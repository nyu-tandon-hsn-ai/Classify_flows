{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import keras\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = np.asarray(df[name], dtype = np.float).mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = np.asarray(df[name], dtype = np.float).std()\n",
    "\n",
    "    df[name] = (np.asarray(df[name], dtype = np.float) - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read df_ 1769 rows.\n",
      "Read df_test 759 rows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "\n",
    "df_ = pd.read_csv(\"./train_70%.csv\")\n",
    "df_test = pd.read_csv(\"./test_30%.csv\")\n",
    "print(\"Read df_ {} rows.\".format(len(df_)))\n",
    "print(\"Read df_test {} rows.\".format(len(df_test)))\n",
    "#print(\"Read {} rows.\".format(len(df1)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df_ = df_.drop(df_.columns[0], axis=1)\n",
    "df_test = df_test.drop(df_test.columns[0], axis=1)\n",
    "\n",
    "df_.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "df_test.dropna(inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stddev(pkt_len)</th>\n",
       "      <th>fb_ratio</th>\n",
       "      <th>inter_arrival_time</th>\n",
       "      <th>pkt_count</th>\n",
       "      <th>is_tcp</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>604.976342</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.013149</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>skype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>384.856909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>skype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>skype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>tencent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stddev(pkt_len)  fb_ratio  inter_arrival_time  pkt_count  is_tcp   outcome\n",
       "0       604.976342       0.9            0.013149         19       1     skype\n",
       "1       384.856909       1.0            0.025902         14       1     skype\n",
       "2         0.000000      -1.0            0.000186       1000       1  download\n",
       "3        -1.000000       0.0           -1.000000          1       0     skype\n",
       "4        -1.000000       0.0           -1.000000          1       0   tencent"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # # Now encode the feature vector\n",
    "\n",
    "# # # encode_text_dummy(df, 'protocol_type')\n",
    "# # encode_numeric_zscore(df, 'forw_byte')\n",
    "# # encode_numeric_zscore(df, 'back_byte')\n",
    "# # encode_numeric_zscore(df, 'tot_byte')\n",
    "\n",
    "# encode_numeric_zscore(df_, ' Flow Duration')\n",
    "# encode_numeric_zscore(df_, ' Total Fwd Packets')\n",
    "# encode_numeric_zscore(df_, ' Total Backward Packets')\n",
    "# encode_numeric_zscore(df_, 'Total Length of Fwd Packets')\n",
    "# encode_numeric_zscore(df_, ' Total Length of Bwd Packets')\n",
    "# encode_numeric_zscore(df_, ' Fwd Packet Length Max')\n",
    "# encode_numeric_zscore(df_, ' Fwd Packet Length Min')\n",
    "# encode_numeric_zscore(df_, ' Fwd Packet Length Mean')\n",
    "# encode_numeric_zscore(df_, ' Fwd Packet Length Std')\n",
    "# encode_numeric_zscore(df_, 'Bwd Packet Length Max')\n",
    "# encode_numeric_zscore(df_, ' Bwd Packet Length Min')\n",
    "# encode_numeric_zscore(df_, ' Bwd Packet Length Mean')\n",
    "# encode_numeric_zscore(df_, ' Bwd Packet Length Std')\n",
    "# encode_numeric_zscore(df_, ' Min Packet Length')\n",
    "# encode_numeric_zscore(df_, ' Max Packet Length')\n",
    "# encode_numeric_zscore(df_, ' Packet Length Mean')\n",
    "# encode_numeric_zscore(df_, ' Packet Length Std')\n",
    "# encode_numeric_zscore(df_, 'total pkt count')\n",
    "# encode_numeric_zscore(df_, 'total byte count')\n",
    "\n",
    "# encode_numeric_zscore(df_test, ' Flow Duration')\n",
    "# encode_numeric_zscore(df_test, ' Total Fwd Packets')\n",
    "# encode_numeric_zscore(df_test, ' Total Backward Packets')\n",
    "# encode_numeric_zscore(df_test, 'Total Length of Fwd Packets')\n",
    "# encode_numeric_zscore(df_test, ' Total Length of Bwd Packets')\n",
    "# encode_numeric_zscore(df_test, ' Fwd Packet Length Max')\n",
    "# encode_numeric_zscore(df_test, ' Fwd Packet Length Min')\n",
    "# encode_numeric_zscore(df_test, ' Fwd Packet Length Mean')\n",
    "# encode_numeric_zscore(df_test, ' Fwd Packet Length Std')\n",
    "# encode_numeric_zscore(df_test, 'Bwd Packet Length Max')\n",
    "# encode_numeric_zscore(df_test, ' Bwd Packet Length Min')\n",
    "# encode_numeric_zscore(df_test, ' Bwd Packet Length Mean')\n",
    "# encode_numeric_zscore(df_test, ' Bwd Packet Length Std')\n",
    "# encode_numeric_zscore(df_test, ' Min Packet Length')\n",
    "# encode_numeric_zscore(df_test, ' Max Packet Length')\n",
    "# encode_numeric_zscore(df_test, ' Packet Length Mean')\n",
    "# encode_numeric_zscore(df_test, ' Packet Length Std')\n",
    "# encode_numeric_zscore(df_test, 'total pkt count')\n",
    "# encode_numeric_zscore(df_test, 'total byte count')\n",
    "\n",
    "# encode_text_index(df_, ' Label')\n",
    "# encode_text_index(df_test, ' Label')\n",
    "# # num_classes = len(outcomes)\n",
    "\n",
    "# # # display 5 rows\n",
    "\n",
    "# # df.dropna(inplace=True,axis=1)\n",
    "# # df[0:5]\n",
    "# # # This is the numeric feature vector, as it goes to the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LoL', 'bili', 'download', 'netease', 'skype', 'tencent', 'youtube'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_numeric_zscore(df_, 'fb_ratio')\n",
    "encode_numeric_zscore(df_, 'pkt_count')\n",
    "encode_numeric_zscore(df_, 'inter_arrival_time')\n",
    "encode_numeric_zscore(df_, 'stddev(pkt_len)')\n",
    "encode_text_index(df_, 'outcome')\n",
    "\n",
    "encode_numeric_zscore(df_test, 'fb_ratio')\n",
    "encode_numeric_zscore(df_test, 'pkt_count')\n",
    "encode_numeric_zscore(df_test, 'inter_arrival_time')\n",
    "encode_numeric_zscore(df_test, 'stddev(pkt_len)')\n",
    "encode_text_index(df_test, 'outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = to_xy(df_, 'outcome')\n",
    "x_test, y_test = to_xy(df_test, 'outcome')\n",
    "# x, y = to_xy(df_, ' Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stddev(pkt_len)</th>\n",
       "      <th>fb_ratio</th>\n",
       "      <th>inter_arrival_time</th>\n",
       "      <th>pkt_count</th>\n",
       "      <th>is_tcp</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.976651</td>\n",
       "      <td>0.826805</td>\n",
       "      <td>-0.114249</td>\n",
       "      <td>-0.054208</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.954096</td>\n",
       "      <td>0.932452</td>\n",
       "      <td>-0.113765</td>\n",
       "      <td>-0.054785</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.833740</td>\n",
       "      <td>-1.180480</td>\n",
       "      <td>-0.114741</td>\n",
       "      <td>0.059085</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.838385</td>\n",
       "      <td>-0.124014</td>\n",
       "      <td>-0.152726</td>\n",
       "      <td>-0.056287</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.838385</td>\n",
       "      <td>-0.124014</td>\n",
       "      <td>-0.152726</td>\n",
       "      <td>-0.056287</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stddev(pkt_len)  fb_ratio  inter_arrival_time  pkt_count  is_tcp  outcome\n",
       "0         1.976651  0.826805           -0.114249  -0.054208       1        4\n",
       "1         0.954096  0.932452           -0.113765  -0.054785       1        4\n",
       "2        -0.833740 -1.180480           -0.114741   0.059085       1        2\n",
       "3        -0.838385 -0.124014           -0.152726  -0.056287       0        4\n",
       "4        -0.838385 -0.124014           -0.152726  -0.056287       0        5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create neural net\n",
    "model = Sequential()\n",
    "\n",
    "# Used relu for activation function\n",
    "# model.add(Dense(4, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dense(8, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dense(4, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# model.add(Dense(5, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(16, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dense(32, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(y_train.shape[1],activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                96        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 119       \n",
      "=================================================================\n",
      "Total params: 215\n",
      "Trainable params: 215\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1769 samples, validate on 759 samples\n",
      "Epoch 1/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.8010 - acc: 0.4211 - val_loss: 1.6635 - val_acc: 0.4638\n",
      "Epoch 2/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.5628 - acc: 0.4777 - val_loss: 1.4940 - val_acc: 0.4638\n",
      "Epoch 3/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.4458 - acc: 0.4845 - val_loss: 1.4157 - val_acc: 0.4901\n",
      "Epoch 4/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.3737 - acc: 0.5269 - val_loss: 1.3522 - val_acc: 0.4980\n",
      "Epoch 5/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.3182 - acc: 0.5370 - val_loss: 1.3030 - val_acc: 0.5152\n",
      "Epoch 6/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.2763 - acc: 0.5353 - val_loss: 1.2745 - val_acc: 0.5152\n",
      "Epoch 7/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.2489 - acc: 0.5421 - val_loss: 1.2532 - val_acc: 0.5152\n",
      "Epoch 8/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.2302 - acc: 0.5387 - val_loss: 1.2392 - val_acc: 0.5165\n",
      "Epoch 9/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.2156 - acc: 0.5370 - val_loss: 1.2268 - val_acc: 0.5165\n",
      "Epoch 10/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.2026 - acc: 0.5370 - val_loss: 1.2240 - val_acc: 0.5178\n",
      "Epoch 11/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.1913 - acc: 0.5308 - val_loss: 1.2207 - val_acc: 0.5204\n",
      "Epoch 12/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.1792 - acc: 0.5302 - val_loss: 1.2077 - val_acc: 0.5455\n",
      "Epoch 13/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.1681 - acc: 0.5370 - val_loss: 1.2165 - val_acc: 0.5455\n",
      "Epoch 14/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.1576 - acc: 0.5557 - val_loss: 1.2264 - val_acc: 0.5494\n",
      "Epoch 15/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.1492 - acc: 0.5715 - val_loss: 1.2356 - val_acc: 0.5428\n",
      "Epoch 16/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.1392 - acc: 0.5681 - val_loss: 1.2476 - val_acc: 0.5362\n",
      "Epoch 17/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.1308 - acc: 0.5732 - val_loss: 1.2668 - val_acc: 0.5362\n",
      "Epoch 18/1000\n",
      "1769/1769 [==============================] - 0s - loss: 1.1211 - acc: 0.5800 - val_loss: 1.2911 - val_acc: 0.5375\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120c6ada0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "batch_size = 10\n",
    "epochs = 1000\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=1,batch_size = batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 0.5375494071146245\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_eval[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iff = np.ones(272618, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iff = 3*iff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 0.0158  0.0013  0.0264  0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.0448  0.      0.0382  0.      0.    ]\n",
      " [ 0.      0.      0.1212  0.      0.0013  0.      0.    ]\n",
      " [ 0.      0.0013  0.      0.      0.0382  0.0119  0.    ]\n",
      " [ 0.      0.      0.      0.      0.3715  0.004   0.    ]\n",
      " [ 0.      0.0105  0.0105  0.      0.1528  0.029   0.    ]\n",
      " [ 0.      0.      0.0184  0.      0.0988  0.004   0.    ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "Cm = confusion_matrix(y_eval,pred)\n",
    "C = np.sum(Cm)\n",
    "Cm = Cm/C\n",
    "print('Confusion Matrix:')\n",
    "print(np.array_str(Cm, precision=4, suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# print('DDos precision & recall & f1_score & support:')\n",
    "# typ = [ \"precision\", \"recall\", \"f1_score\", \"support\"]\n",
    "# ddos_prfs = precision_recall_fscore_support(y_eval, pred)\n",
    "# ddos_list = [x for x,_ in ddos_prfs]\n",
    "# for f, b in zip(typ, ddos_list):\n",
    "#     print( f, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normal precision & recall & f1_score & support:')\n",
    "typ = [ \"precision\", \"recall\", \"f1_score\", \"support\"]\n",
    "ddos_prfs = precision_recall_fscore_support(y_eval, pred)\n",
    "ddos_list = [_ for x,_ in ddos_prfs]\n",
    "for f, b in zip(typ, ddos_list):\n",
    "    print( f, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valAcc = history_cb.val_acc\n",
    "valLoss = history_cb.val_loss\n",
    "epoch_it = np.arange(1,16)\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch_it, valAcc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.tight_layout()\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch_it, valLoss)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ls = history_cb.losses\n",
    "Acc = history_cb.accs\n",
    "ntr = np.shape(x_train)[0]\n",
    "epochNum = []\n",
    "for i in range(5040):\n",
    "    epochNum.append(i*100/ntr)\n",
    "\n",
    "plt.subplot(122)\n",
    "matplotlib.pyplot.semilogy(epochNum, Ls)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.subplot(121)\n",
    "matplotlib.pyplot.semilogy(epochNum, Acc)\n",
    "# plt.plot(epochNum, Acc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.tight_layout()\n",
    "# plt.axis([0.3,3,0.96,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.pyplot.semilogy(epochNum, Acc)\n",
    "# plt.plot(epochNum, Acc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.tight_layout()\n",
    "# plt.axis([0.3,3,0.96,1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# model.save('model_trained_nosampled_sampling@100%.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model\n",
    "\n",
    "# # returns a compiled model\n",
    "# # identical to the previous one\n",
    "# model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hist = model.fit(x, y, validation_split=0.2)\n",
    "# print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.shape(hist.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model2 = load_model('model_trained_sampling@100%.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Measure accuracy\n",
    "# pred = model2.predict(x_test)\n",
    "# pred = np.argmax(pred,axis=1)\n",
    "# y_eval = np.argmax(y_test,axis=1)\n",
    "# score = metrics.accuracy_score(y_eval, pred)\n",
    "# print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
