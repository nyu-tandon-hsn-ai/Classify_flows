{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import keras\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = np.asarray(df[name], dtype = np.float).mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = np.asarray(df[name], dtype = np.float).std()\n",
    "\n",
    "    df[name] = (np.asarray(df[name], dtype = np.float) - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read df_ 3378 rows.\n",
      "Read df_test 1448 rows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "\n",
    "df_ = pd.read_csv(\"./train_70%.csv\")\n",
    "df_test = pd.read_csv(\"./test_30%.csv\")\n",
    "print(\"Read df_ {} rows.\".format(len(df_)))\n",
    "print(\"Read df_test {} rows.\".format(len(df_test)))\n",
    "#print(\"Read {} rows.\".format(len(df1)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df_ = df_.drop(df_.columns[0], axis=1)\n",
    "df_test = df_test.drop(df_test.columns[0], axis=1)\n",
    "\n",
    "df_.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "df_test.dropna(inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg(pkt_len)</th>\n",
       "      <th>stddev(pkt_len)</th>\n",
       "      <th>fb_ratio</th>\n",
       "      <th>inter_arrival_time</th>\n",
       "      <th>pkt_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>pkt_len</th>\n",
       "      <th>is_tcp</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>850.765612</td>\n",
       "      <td>345.487167</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>1233</td>\n",
       "      <td>3.526154</td>\n",
       "      <td>1048994.0</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170.785714</td>\n",
       "      <td>384.539321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019311</td>\n",
       "      <td>14</td>\n",
       "      <td>0.251042</td>\n",
       "      <td>2391.0</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170.785714</td>\n",
       "      <td>384.539321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021926</td>\n",
       "      <td>14</td>\n",
       "      <td>0.285042</td>\n",
       "      <td>2391.0</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.323529</td>\n",
       "      <td>350.338167</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.194919</td>\n",
       "      <td>34</td>\n",
       "      <td>6.432321</td>\n",
       "      <td>6539.0</td>\n",
       "      <td>1</td>\n",
       "      <td>streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>837.180556</td>\n",
       "      <td>667.089557</td>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>936</td>\n",
       "      <td>1.107098</td>\n",
       "      <td>783601.0</td>\n",
       "      <td>1</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1320.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>795</td>\n",
       "      <td>0.067596</td>\n",
       "      <td>1049400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>107.291667</td>\n",
       "      <td>199.128641</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.244188</td>\n",
       "      <td>24</td>\n",
       "      <td>5.616323</td>\n",
       "      <td>2575.0</td>\n",
       "      <td>1</td>\n",
       "      <td>streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016523</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>170.785714</td>\n",
       "      <td>384.539321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019459</td>\n",
       "      <td>14</td>\n",
       "      <td>0.252972</td>\n",
       "      <td>2391.0</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>831.532488</td>\n",
       "      <td>344.903819</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>1262</td>\n",
       "      <td>3.638251</td>\n",
       "      <td>1049394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>166.653846</td>\n",
       "      <td>359.077701</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>26</td>\n",
       "      <td>0.175460</td>\n",
       "      <td>4333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1450.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>724</td>\n",
       "      <td>0.138326</td>\n",
       "      <td>1049800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1320.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>795</td>\n",
       "      <td>0.068551</td>\n",
       "      <td>1049400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.419869</td>\n",
       "      <td>4</td>\n",
       "      <td>1.259606</td>\n",
       "      <td>416.0</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50.444444</td>\n",
       "      <td>76.983743</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.813895</td>\n",
       "      <td>18</td>\n",
       "      <td>30.836216</td>\n",
       "      <td>908.0</td>\n",
       "      <td>1</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>170.040000</td>\n",
       "      <td>398.592574</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>0.220641</td>\n",
       "      <td>25</td>\n",
       "      <td>5.295377</td>\n",
       "      <td>4251.0</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>400.157895</td>\n",
       "      <td>595.732533</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>19</td>\n",
       "      <td>0.323139</td>\n",
       "      <td>7603.0</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>189.900000</td>\n",
       "      <td>360.822040</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092944</td>\n",
       "      <td>10</td>\n",
       "      <td>0.836496</td>\n",
       "      <td>1899.0</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    avg(pkt_len)  stddev(pkt_len)  fb_ratio  inter_arrival_time  pkt_count  \\\n",
       "0     850.765612       345.487167 -1.000000            0.002862       1233   \n",
       "1     170.785714       384.539321  1.000000            0.019311         14   \n",
       "2     170.785714       384.539321  1.000000            0.021926         14   \n",
       "3     192.323529       350.338167  1.125000            0.194919         34   \n",
       "4     837.180556       667.089557  0.668449            0.001184        936   \n",
       "5    1320.000000         0.000000 -1.000000            0.000085        795   \n",
       "6      51.000000        -1.000000  0.000000           -1.000000          1   \n",
       "7     107.291667       199.128641  1.181818            0.244188         24   \n",
       "8      68.000000        -1.000000  0.000000           -1.000000          1   \n",
       "9     170.785714       384.539321  1.000000            0.019459         14   \n",
       "10    831.532488       344.903819 -1.000000            0.002885       1262   \n",
       "11    166.653846       359.077701  1.166667            0.007018         26   \n",
       "12   1450.000000         0.000000 -1.000000            0.000191        724   \n",
       "13   1320.000000         0.000000 -1.000000            0.000086        795   \n",
       "14    104.000000         0.000000 -1.000000            0.419869          4   \n",
       "15     50.444444        76.983743 -1.000000            1.813895         18   \n",
       "16    170.040000       398.592574  1.083333            0.220641         25   \n",
       "17    400.157895       595.732533  0.900000            0.017952         19   \n",
       "18     32.000000         0.000000  1.000000            0.009942          2   \n",
       "19    189.900000       360.822040  1.000000            0.092944         10   \n",
       "\n",
       "     duration    pkt_len  is_tcp    outcome  \n",
       "0    3.526154  1048994.0       0       voip  \n",
       "1    0.251042     2391.0       1       voip  \n",
       "2    0.285042     2391.0       1       voip  \n",
       "3    6.432321     6539.0       1  streaming  \n",
       "4    1.107098   783601.0       1       game  \n",
       "5    0.067596  1049400.0       1   download  \n",
       "6    0.003002       51.0       0       game  \n",
       "7    5.616323     2575.0       1  streaming  \n",
       "8    0.016523       68.0       0       game  \n",
       "9    0.252972     2391.0       1       voip  \n",
       "10   3.638251  1049394.0       0       voip  \n",
       "11   0.175460     4333.0       1       voip  \n",
       "12   0.138326  1049800.0       1   download  \n",
       "13   0.068551  1049400.0       1   download  \n",
       "14   1.259606      416.0       0       voip  \n",
       "15  30.836216      908.0       1       game  \n",
       "16   5.295377     4251.0       1       voip  \n",
       "17   0.323139     7603.0       1       voip  \n",
       "18   0.009942       64.0       1       voip  \n",
       "19   0.836496     1899.0       1       voip  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['download', 'game', 'streaming', 'voip'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_numeric_zscore(df_, 'fb_ratio')\n",
    "encode_numeric_zscore(df_, 'pkt_count')\n",
    "encode_numeric_zscore(df_, 'inter_arrival_time')\n",
    "encode_numeric_zscore(df_, 'stddev(pkt_len)')\n",
    "encode_numeric_zscore(df_, 'avg(pkt_len)')\n",
    "encode_numeric_zscore(df_, 'pkt_len')\n",
    "encode_numeric_zscore(df_, 'duration')\n",
    "encode_text_index(df_, 'outcome')\n",
    "\n",
    "encode_numeric_zscore(df_test, 'fb_ratio')\n",
    "encode_numeric_zscore(df_test, 'pkt_count')\n",
    "encode_numeric_zscore(df_test, 'inter_arrival_time')\n",
    "encode_numeric_zscore(df_test, 'stddev(pkt_len)')\n",
    "encode_numeric_zscore(df_test, 'avg(pkt_len)')\n",
    "encode_numeric_zscore(df_test, 'pkt_len')\n",
    "encode_numeric_zscore(df_test, 'duration')\n",
    "encode_text_index(df_test, 'outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = to_xy(df_, 'outcome')\n",
    "x_test, y_test = to_xy(df_test, 'outcome')\n",
    "# x, y = to_xy(df_, ' Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg(pkt_len)</th>\n",
       "      <th>stddev(pkt_len)</th>\n",
       "      <th>fb_ratio</th>\n",
       "      <th>inter_arrival_time</th>\n",
       "      <th>pkt_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>pkt_len</th>\n",
       "      <th>is_tcp</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.207890</td>\n",
       "      <td>0.316372</td>\n",
       "      <td>-1.610124</td>\n",
       "      <td>-0.096689</td>\n",
       "      <td>0.929420</td>\n",
       "      <td>-0.162339</td>\n",
       "      <td>1.944402</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.492414</td>\n",
       "      <td>0.502793</td>\n",
       "      <td>0.681685</td>\n",
       "      <td>-0.095893</td>\n",
       "      <td>-0.294377</td>\n",
       "      <td>-0.185909</td>\n",
       "      <td>-0.524746</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.492414</td>\n",
       "      <td>0.502793</td>\n",
       "      <td>0.681685</td>\n",
       "      <td>-0.095766</td>\n",
       "      <td>-0.294377</td>\n",
       "      <td>-0.185664</td>\n",
       "      <td>-0.524746</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.438559</td>\n",
       "      <td>0.339529</td>\n",
       "      <td>0.824923</td>\n",
       "      <td>-0.087389</td>\n",
       "      <td>-0.274298</td>\n",
       "      <td>-0.141425</td>\n",
       "      <td>-0.514960</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.173920</td>\n",
       "      <td>1.851588</td>\n",
       "      <td>0.301759</td>\n",
       "      <td>-0.096771</td>\n",
       "      <td>0.631251</td>\n",
       "      <td>-0.179748</td>\n",
       "      <td>1.318287</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.381220</td>\n",
       "      <td>-1.332861</td>\n",
       "      <td>-1.610124</td>\n",
       "      <td>-0.096824</td>\n",
       "      <td>0.489696</td>\n",
       "      <td>-0.187229</td>\n",
       "      <td>1.945360</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.791941</td>\n",
       "      <td>-1.337635</td>\n",
       "      <td>-0.464220</td>\n",
       "      <td>-0.145254</td>\n",
       "      <td>-0.307428</td>\n",
       "      <td>-0.187694</td>\n",
       "      <td>-0.530267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.651183</td>\n",
       "      <td>-0.382291</td>\n",
       "      <td>0.890031</td>\n",
       "      <td>-0.085003</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.147297</td>\n",
       "      <td>-0.524312</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.749432</td>\n",
       "      <td>-1.337635</td>\n",
       "      <td>-0.464220</td>\n",
       "      <td>-0.145254</td>\n",
       "      <td>-0.307428</td>\n",
       "      <td>-0.187596</td>\n",
       "      <td>-0.530227</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.492414</td>\n",
       "      <td>0.502793</td>\n",
       "      <td>0.681685</td>\n",
       "      <td>-0.095886</td>\n",
       "      <td>-0.294377</td>\n",
       "      <td>-0.185895</td>\n",
       "      <td>-0.524746</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.159797</td>\n",
       "      <td>0.313587</td>\n",
       "      <td>-1.610124</td>\n",
       "      <td>-0.096688</td>\n",
       "      <td>0.958534</td>\n",
       "      <td>-0.161532</td>\n",
       "      <td>1.945346</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.502746</td>\n",
       "      <td>0.381249</td>\n",
       "      <td>0.872669</td>\n",
       "      <td>-0.096488</td>\n",
       "      <td>-0.282329</td>\n",
       "      <td>-0.186452</td>\n",
       "      <td>-0.520165</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.706288</td>\n",
       "      <td>-1.332861</td>\n",
       "      <td>-1.610124</td>\n",
       "      <td>-0.096819</td>\n",
       "      <td>0.418417</td>\n",
       "      <td>-0.186720</td>\n",
       "      <td>1.946304</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.381220</td>\n",
       "      <td>-1.332861</td>\n",
       "      <td>-1.610124</td>\n",
       "      <td>-0.096824</td>\n",
       "      <td>0.489696</td>\n",
       "      <td>-0.187222</td>\n",
       "      <td>1.945360</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.659413</td>\n",
       "      <td>-1.332861</td>\n",
       "      <td>-1.610124</td>\n",
       "      <td>-0.076495</td>\n",
       "      <td>-0.304416</td>\n",
       "      <td>-0.178650</td>\n",
       "      <td>-0.529406</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.793330</td>\n",
       "      <td>-0.965368</td>\n",
       "      <td>-1.610124</td>\n",
       "      <td>-0.008989</td>\n",
       "      <td>-0.290361</td>\n",
       "      <td>0.034199</td>\n",
       "      <td>-0.528245</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.494279</td>\n",
       "      <td>0.569879</td>\n",
       "      <td>0.777177</td>\n",
       "      <td>-0.086143</td>\n",
       "      <td>-0.283333</td>\n",
       "      <td>-0.149607</td>\n",
       "      <td>-0.520358</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.081136</td>\n",
       "      <td>1.510955</td>\n",
       "      <td>0.567095</td>\n",
       "      <td>-0.095959</td>\n",
       "      <td>-0.289357</td>\n",
       "      <td>-0.185390</td>\n",
       "      <td>-0.512450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.839451</td>\n",
       "      <td>-1.332861</td>\n",
       "      <td>0.681685</td>\n",
       "      <td>-0.096346</td>\n",
       "      <td>-0.306424</td>\n",
       "      <td>-0.187644</td>\n",
       "      <td>-0.530236</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.444619</td>\n",
       "      <td>0.389575</td>\n",
       "      <td>0.681685</td>\n",
       "      <td>-0.092327</td>\n",
       "      <td>-0.298392</td>\n",
       "      <td>-0.181695</td>\n",
       "      <td>-0.525907</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    avg(pkt_len)  stddev(pkt_len)  fb_ratio  inter_arrival_time  pkt_count  \\\n",
       "0       1.207890         0.316372 -1.610124           -0.096689   0.929420   \n",
       "1      -0.492414         0.502793  0.681685           -0.095893  -0.294377   \n",
       "2      -0.492414         0.502793  0.681685           -0.095766  -0.294377   \n",
       "3      -0.438559         0.339529  0.824923           -0.087389  -0.274298   \n",
       "4       1.173920         1.851588  0.301759           -0.096771   0.631251   \n",
       "5       2.381220        -1.332861 -1.610124           -0.096824   0.489696   \n",
       "6      -0.791941        -1.337635 -0.464220           -0.145254  -0.307428   \n",
       "7      -0.651183        -0.382291  0.890031           -0.085003  -0.284337   \n",
       "8      -0.749432        -1.337635 -0.464220           -0.145254  -0.307428   \n",
       "9      -0.492414         0.502793  0.681685           -0.095886  -0.294377   \n",
       "10      1.159797         0.313587 -1.610124           -0.096688   0.958534   \n",
       "11     -0.502746         0.381249  0.872669           -0.096488  -0.282329   \n",
       "12      2.706288        -1.332861 -1.610124           -0.096819   0.418417   \n",
       "13      2.381220        -1.332861 -1.610124           -0.096824   0.489696   \n",
       "14     -0.659413        -1.332861 -1.610124           -0.076495  -0.304416   \n",
       "15     -0.793330        -0.965368 -1.610124           -0.008989  -0.290361   \n",
       "16     -0.494279         0.569879  0.777177           -0.086143  -0.283333   \n",
       "17      0.081136         1.510955  0.567095           -0.095959  -0.289357   \n",
       "18     -0.839451        -1.332861  0.681685           -0.096346  -0.306424   \n",
       "19     -0.444619         0.389575  0.681685           -0.092327  -0.298392   \n",
       "\n",
       "    duration   pkt_len  is_tcp  outcome  \n",
       "0  -0.162339  1.944402       0        3  \n",
       "1  -0.185909 -0.524746       1        3  \n",
       "2  -0.185664 -0.524746       1        3  \n",
       "3  -0.141425 -0.514960       1        2  \n",
       "4  -0.179748  1.318287       1        1  \n",
       "5  -0.187229  1.945360       1        0  \n",
       "6  -0.187694 -0.530267       0        1  \n",
       "7  -0.147297 -0.524312       1        2  \n",
       "8  -0.187596 -0.530227       0        1  \n",
       "9  -0.185895 -0.524746       1        3  \n",
       "10 -0.161532  1.945346       0        3  \n",
       "11 -0.186452 -0.520165       1        3  \n",
       "12 -0.186720  1.946304       1        0  \n",
       "13 -0.187222  1.945360       1        0  \n",
       "14 -0.178650 -0.529406       0        3  \n",
       "15  0.034199 -0.528245       1        1  \n",
       "16 -0.149607 -0.520358       1        3  \n",
       "17 -0.185390 -0.512450       1        3  \n",
       "18 -0.187644 -0.530236       1        3  \n",
       "19 -0.181695 -0.525907       1        3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(y_train.shape[1],activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 212\n",
      "Trainable params: 212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # TODO:  Create two empty lists, self.loss and self.val_acc\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.val_acc = []\n",
    "        self.val_loss = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # TODO:  This is called at the end of each batch.  \n",
    "        # Add the loss in logs.get('loss') to the loss list\n",
    "        loss = logs.get('loss')\n",
    "        acc = logs.get('acc')\n",
    "        self.losses.append(loss)\n",
    "        self.accs.append(acc)\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # TODO:  This is called at the end of each epoch.  \n",
    "        # Add the test accuracy in logs.get('loss') to the val_acc list\n",
    "        val_acc = logs.get('val_acc')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        self.val_acc.append(val_acc)\n",
    "        self.val_loss.append(val_loss)\n",
    "\n",
    "# Create an instance of the history callback\n",
    "history_cb = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3378 samples, validate on 1448 samples\n",
      "Epoch 1/300\n",
      "3378/3378 [==============================] - 0s - loss: 1.2527 - acc: 0.6341 - val_loss: 1.1134 - val_acc: 0.6644\n",
      "Epoch 2/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.9925 - acc: 0.6835 - val_loss: 0.9205 - val_acc: 0.6809\n",
      "Epoch 3/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.8475 - acc: 0.7108 - val_loss: 0.8484 - val_acc: 0.7155\n",
      "Epoch 4/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.7906 - acc: 0.7339 - val_loss: 0.8155 - val_acc: 0.7175\n",
      "Epoch 5/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.7576 - acc: 0.7374 - val_loss: 0.7925 - val_acc: 0.7134\n",
      "Epoch 6/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.7353 - acc: 0.7430 - val_loss: 0.7760 - val_acc: 0.7169\n",
      "Epoch 7/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.7192 - acc: 0.7487 - val_loss: 0.7662 - val_acc: 0.7203\n",
      "Epoch 8/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.7073 - acc: 0.7454 - val_loss: 0.7573 - val_acc: 0.7258\n",
      "Epoch 9/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6984 - acc: 0.7481 - val_loss: 0.7501 - val_acc: 0.7175\n",
      "Epoch 10/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6898 - acc: 0.7493 - val_loss: 0.7448 - val_acc: 0.7238\n",
      "Epoch 11/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6840 - acc: 0.7534 - val_loss: 0.7389 - val_acc: 0.7182\n",
      "Epoch 12/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6758 - acc: 0.7549 - val_loss: 0.7311 - val_acc: 0.7224\n",
      "Epoch 13/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6693 - acc: 0.7590 - val_loss: 0.7248 - val_acc: 0.7300\n",
      "Epoch 14/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6623 - acc: 0.7626 - val_loss: 0.7204 - val_acc: 0.7355\n",
      "Epoch 15/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6558 - acc: 0.7617 - val_loss: 0.7140 - val_acc: 0.7396\n",
      "Epoch 16/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6495 - acc: 0.7644 - val_loss: 0.7107 - val_acc: 0.7403\n",
      "Epoch 17/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6436 - acc: 0.7647 - val_loss: 0.7049 - val_acc: 0.7369\n",
      "Epoch 18/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6386 - acc: 0.7655 - val_loss: 0.7042 - val_acc: 0.7376\n",
      "Epoch 19/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6338 - acc: 0.7664 - val_loss: 0.7002 - val_acc: 0.7376\n",
      "Epoch 20/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6292 - acc: 0.7667 - val_loss: 0.6936 - val_acc: 0.7390\n",
      "Epoch 21/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6247 - acc: 0.7673 - val_loss: 0.6943 - val_acc: 0.7410\n",
      "Epoch 22/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6209 - acc: 0.7676 - val_loss: 0.6887 - val_acc: 0.7431\n",
      "Epoch 23/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6171 - acc: 0.7688 - val_loss: 0.6888 - val_acc: 0.7438\n",
      "Epoch 24/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6138 - acc: 0.7732 - val_loss: 0.6858 - val_acc: 0.7417\n",
      "Epoch 25/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6116 - acc: 0.7718 - val_loss: 0.6785 - val_acc: 0.7500\n",
      "Epoch 26/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6064 - acc: 0.7715 - val_loss: 0.6819 - val_acc: 0.7445\n",
      "Epoch 27/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.6017 - acc: 0.7718 - val_loss: 0.6769 - val_acc: 0.7445\n",
      "Epoch 28/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5991 - acc: 0.7744 - val_loss: 0.6763 - val_acc: 0.7445\n",
      "Epoch 29/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5964 - acc: 0.7756 - val_loss: 0.6722 - val_acc: 0.7452\n",
      "Epoch 30/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5947 - acc: 0.7724 - val_loss: 0.6756 - val_acc: 0.7583\n",
      "Epoch 31/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5912 - acc: 0.7771 - val_loss: 0.6751 - val_acc: 0.7452\n",
      "Epoch 32/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5898 - acc: 0.7726 - val_loss: 0.6756 - val_acc: 0.7445\n",
      "Epoch 33/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5875 - acc: 0.7753 - val_loss: 0.6754 - val_acc: 0.7659\n",
      "Epoch 34/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5860 - acc: 0.7792 - val_loss: 0.6645 - val_acc: 0.7535\n",
      "Epoch 35/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5836 - acc: 0.7783 - val_loss: 0.6612 - val_acc: 0.7659\n",
      "Epoch 36/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5816 - acc: 0.7750 - val_loss: 0.6700 - val_acc: 0.7521\n",
      "Epoch 37/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5789 - acc: 0.7792 - val_loss: 0.6583 - val_acc: 0.7528\n",
      "Epoch 38/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5772 - acc: 0.7792 - val_loss: 0.6579 - val_acc: 0.7742\n",
      "Epoch 39/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5769 - acc: 0.7842 - val_loss: 0.6623 - val_acc: 0.7535\n",
      "Epoch 40/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5742 - acc: 0.7821 - val_loss: 0.6569 - val_acc: 0.7769\n",
      "Epoch 41/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5737 - acc: 0.7851 - val_loss: 0.6560 - val_acc: 0.7776\n",
      "Epoch 42/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5713 - acc: 0.7845 - val_loss: 0.6505 - val_acc: 0.7728\n",
      "Epoch 43/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5697 - acc: 0.7889 - val_loss: 0.6564 - val_acc: 0.7742\n",
      "Epoch 44/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5674 - acc: 0.7874 - val_loss: 0.6609 - val_acc: 0.7714\n",
      "Epoch 45/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5673 - acc: 0.7889 - val_loss: 0.6563 - val_acc: 0.7783\n",
      "Epoch 46/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5646 - acc: 0.7919 - val_loss: 0.6548 - val_acc: 0.7756\n",
      "Epoch 47/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5634 - acc: 0.7925 - val_loss: 0.6509 - val_acc: 0.7797\n",
      "Epoch 48/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5620 - acc: 0.7940 - val_loss: 0.6540 - val_acc: 0.7728\n",
      "Epoch 49/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5617 - acc: 0.7940 - val_loss: 0.6470 - val_acc: 0.7783\n",
      "Epoch 50/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5599 - acc: 0.7937 - val_loss: 0.6488 - val_acc: 0.7776\n",
      "Epoch 51/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5591 - acc: 0.7951 - val_loss: 0.6494 - val_acc: 0.7769\n",
      "Epoch 52/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5574 - acc: 0.7948 - val_loss: 0.6483 - val_acc: 0.7769\n",
      "Epoch 53/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5560 - acc: 0.7943 - val_loss: 0.6581 - val_acc: 0.7700\n",
      "Epoch 54/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5554 - acc: 0.7975 - val_loss: 0.6479 - val_acc: 0.7776\n",
      "Epoch 55/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5546 - acc: 0.7984 - val_loss: 0.6423 - val_acc: 0.7783\n",
      "Epoch 56/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5522 - acc: 0.7975 - val_loss: 0.6537 - val_acc: 0.7735\n",
      "Epoch 57/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5526 - acc: 0.7946 - val_loss: 0.6412 - val_acc: 0.7790\n",
      "Epoch 58/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5501 - acc: 0.7963 - val_loss: 0.6476 - val_acc: 0.7707\n",
      "Epoch 59/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5499 - acc: 0.7957 - val_loss: 0.6440 - val_acc: 0.7790\n",
      "Epoch 60/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5481 - acc: 0.7951 - val_loss: 0.6419 - val_acc: 0.7769\n",
      "Epoch 61/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5494 - acc: 0.7978 - val_loss: 0.6475 - val_acc: 0.7707\n",
      "Epoch 62/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5473 - acc: 0.7972 - val_loss: 0.6376 - val_acc: 0.7790\n",
      "Epoch 63/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5467 - acc: 0.7931 - val_loss: 0.6359 - val_acc: 0.7804\n",
      "Epoch 64/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5462 - acc: 0.7946 - val_loss: 0.6446 - val_acc: 0.7804\n",
      "Epoch 65/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3378/3378 [==============================] - 0s - loss: 0.5440 - acc: 0.7940 - val_loss: 0.6422 - val_acc: 0.7790\n",
      "Epoch 66/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5425 - acc: 0.7957 - val_loss: 0.6423 - val_acc: 0.7749\n",
      "Epoch 67/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5423 - acc: 0.7925 - val_loss: 0.6386 - val_acc: 0.7783\n",
      "Epoch 68/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5422 - acc: 0.7963 - val_loss: 0.6281 - val_acc: 0.7825\n",
      "Epoch 69/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5403 - acc: 0.7960 - val_loss: 0.6379 - val_acc: 0.7776\n",
      "Epoch 70/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5408 - acc: 0.7934 - val_loss: 0.6359 - val_acc: 0.7831\n",
      "Epoch 71/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5392 - acc: 0.7966 - val_loss: 0.6351 - val_acc: 0.7811\n",
      "Epoch 72/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5399 - acc: 0.7928 - val_loss: 0.6417 - val_acc: 0.7735\n",
      "Epoch 73/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5369 - acc: 0.7951 - val_loss: 0.6383 - val_acc: 0.7776\n",
      "Epoch 74/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5371 - acc: 0.7948 - val_loss: 0.6301 - val_acc: 0.7831\n",
      "Epoch 75/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5363 - acc: 0.7946 - val_loss: 0.6345 - val_acc: 0.7818\n",
      "Epoch 76/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5348 - acc: 0.7966 - val_loss: 0.6294 - val_acc: 0.7790\n",
      "Epoch 77/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5353 - acc: 0.7951 - val_loss: 0.6372 - val_acc: 0.7742\n",
      "Epoch 78/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5336 - acc: 0.7943 - val_loss: 0.6339 - val_acc: 0.7769\n",
      "Epoch 79/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5332 - acc: 0.7943 - val_loss: 0.6309 - val_acc: 0.7818\n",
      "Epoch 80/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5334 - acc: 0.7940 - val_loss: 0.6368 - val_acc: 0.7797\n",
      "Epoch 81/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5325 - acc: 0.7922 - val_loss: 0.6263 - val_acc: 0.7811\n",
      "Epoch 82/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5317 - acc: 0.7951 - val_loss: 0.6246 - val_acc: 0.7866\n",
      "Epoch 83/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5306 - acc: 0.7931 - val_loss: 0.6177 - val_acc: 0.7749\n",
      "Epoch 84/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5301 - acc: 0.7981 - val_loss: 0.6182 - val_acc: 0.7756\n",
      "Epoch 85/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5301 - acc: 0.7984 - val_loss: 0.6291 - val_acc: 0.7818\n",
      "Epoch 86/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5298 - acc: 0.7984 - val_loss: 0.6235 - val_acc: 0.7831\n",
      "Epoch 87/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5289 - acc: 0.7934 - val_loss: 0.6192 - val_acc: 0.7783\n",
      "Epoch 88/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5280 - acc: 0.7957 - val_loss: 0.6262 - val_acc: 0.7852\n",
      "Epoch 89/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5268 - acc: 0.7960 - val_loss: 0.6268 - val_acc: 0.7880\n",
      "Epoch 90/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5283 - acc: 0.7966 - val_loss: 0.6195 - val_acc: 0.77900.\n",
      "Epoch 91/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5272 - acc: 0.7954 - val_loss: 0.6172 - val_acc: 0.7825\n",
      "Epoch 92/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5257 - acc: 0.7963 - val_loss: 0.6196 - val_acc: 0.7811\n",
      "Epoch 93/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5261 - acc: 0.7946 - val_loss: 0.6271 - val_acc: 0.7887\n",
      "Epoch 94/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5257 - acc: 0.7990 - val_loss: 0.6249 - val_acc: 0.7880\n",
      "Epoch 95/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5241 - acc: 0.7960 - val_loss: 0.6400 - val_acc: 0.7811\n",
      "Epoch 96/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5241 - acc: 0.7972 - val_loss: 0.6212 - val_acc: 0.7866\n",
      "Epoch 97/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5233 - acc: 0.7960 - val_loss: 0.6387 - val_acc: 0.7790\n",
      "Epoch 98/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5240 - acc: 0.7978 - val_loss: 0.6272 - val_acc: 0.7797\n",
      "Epoch 99/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5224 - acc: 0.7954 - val_loss: 0.6098 - val_acc: 0.7790\n",
      "Epoch 100/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5219 - acc: 0.7972 - val_loss: 0.6225 - val_acc: 0.7880\n",
      "Epoch 101/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5222 - acc: 0.7946 - val_loss: 0.6230 - val_acc: 0.7873\n",
      "Epoch 102/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5215 - acc: 0.7957 - val_loss: 0.6109 - val_acc: 0.7838\n",
      "Epoch 103/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5222 - acc: 0.7981 - val_loss: 0.6244 - val_acc: 0.7880\n",
      "Epoch 104/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5202 - acc: 0.7954 - val_loss: 0.6156 - val_acc: 0.7825\n",
      "Epoch 105/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5196 - acc: 0.7969 - val_loss: 0.6265 - val_acc: 0.7887\n",
      "Epoch 106/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5192 - acc: 0.7951 - val_loss: 0.6170 - val_acc: 0.7859\n",
      "Epoch 107/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5191 - acc: 0.7972 - val_loss: 0.6226 - val_acc: 0.7859\n",
      "Epoch 108/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5185 - acc: 0.7984 - val_loss: 0.6157 - val_acc: 0.7797\n",
      "Epoch 109/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5181 - acc: 0.7978 - val_loss: 0.6234 - val_acc: 0.7804\n",
      "Epoch 110/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5194 - acc: 0.7987 - val_loss: 0.6309 - val_acc: 0.7811\n",
      "Epoch 111/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5179 - acc: 0.7993 - val_loss: 0.6157 - val_acc: 0.7818\n",
      "Epoch 112/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5165 - acc: 0.7966 - val_loss: 0.6140 - val_acc: 0.7790\n",
      "Epoch 113/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5168 - acc: 0.7990 - val_loss: 0.6186 - val_acc: 0.7887\n",
      "Epoch 114/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5161 - acc: 0.7978 - val_loss: 0.6193 - val_acc: 0.7880\n",
      "Epoch 115/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5161 - acc: 0.7993 - val_loss: 0.6194 - val_acc: 0.7811\n",
      "Epoch 116/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5164 - acc: 0.7969 - val_loss: 0.6213 - val_acc: 0.7825\n",
      "Epoch 117/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5160 - acc: 0.7966 - val_loss: 0.6139 - val_acc: 0.7845\n",
      "Epoch 118/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5145 - acc: 0.7975 - val_loss: 0.6243 - val_acc: 0.7818\n",
      "Epoch 119/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5148 - acc: 0.8002 - val_loss: 0.6227 - val_acc: 0.7818\n",
      "Epoch 120/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5152 - acc: 0.8002 - val_loss: 0.6142 - val_acc: 0.7831\n",
      "Epoch 121/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5137 - acc: 0.7954 - val_loss: 0.6168 - val_acc: 0.7818\n",
      "Epoch 122/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5139 - acc: 0.8014 - val_loss: 0.6140 - val_acc: 0.7811\n",
      "Epoch 123/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5129 - acc: 0.7993 - val_loss: 0.6172 - val_acc: 0.7859\n",
      "Epoch 124/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5127 - acc: 0.7981 - val_loss: 0.6250 - val_acc: 0.7811\n",
      "Epoch 125/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5126 - acc: 0.8005 - val_loss: 0.6206 - val_acc: 0.7880\n",
      "Epoch 126/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5125 - acc: 0.7981 - val_loss: 0.6235 - val_acc: 0.7818\n",
      "Epoch 127/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5118 - acc: 0.7966 - val_loss: 0.6102 - val_acc: 0.7790\n",
      "Epoch 128/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5112 - acc: 0.7975 - val_loss: 0.6204 - val_acc: 0.7873\n",
      "Epoch 129/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3378/3378 [==============================] - 0s - loss: 0.5121 - acc: 0.8017 - val_loss: 0.6018 - val_acc: 0.7818\n",
      "Epoch 130/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5118 - acc: 0.7993 - val_loss: 0.6150 - val_acc: 0.7769\n",
      "Epoch 131/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5111 - acc: 0.7981 - val_loss: 0.6101 - val_acc: 0.7783\n",
      "Epoch 132/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5106 - acc: 0.8028 - val_loss: 0.6178 - val_acc: 0.7818\n",
      "Epoch 133/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5103 - acc: 0.8017 - val_loss: 0.6075 - val_acc: 0.7776\n",
      "Epoch 134/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5105 - acc: 0.8017 - val_loss: 0.6096 - val_acc: 0.7831\n",
      "Epoch 135/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5093 - acc: 0.7981 - val_loss: 0.6132 - val_acc: 0.7845\n",
      "Epoch 136/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5088 - acc: 0.8022 - val_loss: 0.6108 - val_acc: 0.7776\n",
      "Epoch 137/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5081 - acc: 0.8022 - val_loss: 0.6112 - val_acc: 0.7831\n",
      "Epoch 138/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5084 - acc: 0.8014 - val_loss: 0.6155 - val_acc: 0.7783\n",
      "Epoch 139/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5082 - acc: 0.8020 - val_loss: 0.6085 - val_acc: 0.7783\n",
      "Epoch 140/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5081 - acc: 0.8046 - val_loss: 0.6035 - val_acc: 0.7831\n",
      "Epoch 141/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5067 - acc: 0.8022 - val_loss: 0.6196 - val_acc: 0.7804\n",
      "Epoch 142/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5081 - acc: 0.8025 - val_loss: 0.6059 - val_acc: 0.7776\n",
      "Epoch 143/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5066 - acc: 0.8014 - val_loss: 0.6047 - val_acc: 0.7776\n",
      "Epoch 144/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5060 - acc: 0.8014 - val_loss: 0.6110 - val_acc: 0.7825\n",
      "Epoch 145/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5065 - acc: 0.8025 - val_loss: 0.6108 - val_acc: 0.7831\n",
      "Epoch 146/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5068 - acc: 0.8014 - val_loss: 0.6158 - val_acc: 0.7769\n",
      "Epoch 147/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5050 - acc: 0.8028 - val_loss: 0.6080 - val_acc: 0.7769\n",
      "Epoch 148/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5061 - acc: 0.8022 - val_loss: 0.6161 - val_acc: 0.7762\n",
      "Epoch 149/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5055 - acc: 0.8052 - val_loss: 0.6106 - val_acc: 0.7783\n",
      "Epoch 150/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5049 - acc: 0.8028 - val_loss: 0.6221 - val_acc: 0.7783\n",
      "Epoch 151/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5045 - acc: 0.8031 - val_loss: 0.6077 - val_acc: 0.7735\n",
      "Epoch 152/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5037 - acc: 0.8034 - val_loss: 0.6051 - val_acc: 0.7762\n",
      "Epoch 153/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5041 - acc: 0.8043 - val_loss: 0.6180 - val_acc: 0.7804\n",
      "Epoch 154/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5038 - acc: 0.8028 - val_loss: 0.6113 - val_acc: 0.7790\n",
      "Epoch 155/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5032 - acc: 0.8037 - val_loss: 0.6132 - val_acc: 0.7762\n",
      "Epoch 156/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5029 - acc: 0.8022 - val_loss: 0.6191 - val_acc: 0.7804\n",
      "Epoch 157/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5030 - acc: 0.8079 - val_loss: 0.6135 - val_acc: 0.7735\n",
      "Epoch 158/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5029 - acc: 0.8005 - val_loss: 0.6146 - val_acc: 0.7797\n",
      "Epoch 159/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5030 - acc: 0.8037 - val_loss: 0.6114 - val_acc: 0.7762\n",
      "Epoch 160/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5019 - acc: 0.8058 - val_loss: 0.6118 - val_acc: 0.7811\n",
      "Epoch 161/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5027 - acc: 0.8020 - val_loss: 0.6199 - val_acc: 0.7811\n",
      "Epoch 162/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5015 - acc: 0.8031 - val_loss: 0.6081 - val_acc: 0.7762\n",
      "Epoch 163/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5007 - acc: 0.8052 - val_loss: 0.6042 - val_acc: 0.7776\n",
      "Epoch 164/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5013 - acc: 0.8046 - val_loss: 0.6081 - val_acc: 0.7776\n",
      "Epoch 165/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5002 - acc: 0.8070 - val_loss: 0.6062 - val_acc: 0.7749\n",
      "Epoch 166/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5009 - acc: 0.8058 - val_loss: 0.6161 - val_acc: 0.7783\n",
      "Epoch 167/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5003 - acc: 0.8034 - val_loss: 0.6058 - val_acc: 0.7749\n",
      "Epoch 168/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5002 - acc: 0.8031 - val_loss: 0.6108 - val_acc: 0.7797\n",
      "Epoch 169/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5009 - acc: 0.8037 - val_loss: 0.6158 - val_acc: 0.7797\n",
      "Epoch 170/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4993 - acc: 0.8037 - val_loss: 0.6136 - val_acc: 0.7762\n",
      "Epoch 171/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4990 - acc: 0.8055 - val_loss: 0.6095 - val_acc: 0.7790\n",
      "Epoch 172/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.5003 - acc: 0.8046 - val_loss: 0.6036 - val_acc: 0.7756\n",
      "Epoch 173/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4997 - acc: 0.8049 - val_loss: 0.6123 - val_acc: 0.7749\n",
      "Epoch 174/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4986 - acc: 0.8037 - val_loss: 0.6053 - val_acc: 0.7756\n",
      "Epoch 175/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4994 - acc: 0.8034 - val_loss: 0.5999 - val_acc: 0.7769\n",
      "Epoch 176/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4984 - acc: 0.8055 - val_loss: 0.5962 - val_acc: 0.7756\n",
      "Epoch 177/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4979 - acc: 0.8043 - val_loss: 0.6024 - val_acc: 0.7776\n",
      "Epoch 178/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4984 - acc: 0.8040 - val_loss: 0.6336 - val_acc: 0.7825\n",
      "Epoch 179/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4976 - acc: 0.8058 - val_loss: 0.6074 - val_acc: 0.7749\n",
      "Epoch 180/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4976 - acc: 0.8070 - val_loss: 0.6063 - val_acc: 0.7769\n",
      "Epoch 181/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4963 - acc: 0.8046 - val_loss: 0.6010 - val_acc: 0.7749\n",
      "Epoch 182/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4973 - acc: 0.8046 - val_loss: 0.6156 - val_acc: 0.7797\n",
      "Epoch 183/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4967 - acc: 0.8061 - val_loss: 0.6044 - val_acc: 0.7721\n",
      "Epoch 184/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4962 - acc: 0.8037 - val_loss: 0.6159 - val_acc: 0.7831\n",
      "Epoch 185/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4963 - acc: 0.8064 - val_loss: 0.6169 - val_acc: 0.7783\n",
      "Epoch 186/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4974 - acc: 0.8034 - val_loss: 0.6133 - val_acc: 0.7756\n",
      "Epoch 187/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4950 - acc: 0.8034 - val_loss: 0.6195 - val_acc: 0.7790\n",
      "Epoch 188/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4962 - acc: 0.8043 - val_loss: 0.6093 - val_acc: 0.7762\n",
      "Epoch 189/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4959 - acc: 0.8055 - val_loss: 0.6211 - val_acc: 0.7831\n",
      "Epoch 190/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4950 - acc: 0.8061 - val_loss: 0.6117 - val_acc: 0.7769\n",
      "Epoch 191/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4962 - acc: 0.8046 - val_loss: 0.6089 - val_acc: 0.7728\n",
      "Epoch 192/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4949 - acc: 0.8058 - val_loss: 0.6059 - val_acc: 0.7756\n",
      "Epoch 193/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3378/3378 [==============================] - 0s - loss: 0.4958 - acc: 0.8055 - val_loss: 0.6152 - val_acc: 0.7790\n",
      "Epoch 194/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4947 - acc: 0.8034 - val_loss: 0.5954 - val_acc: 0.7756\n",
      "Epoch 195/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4946 - acc: 0.8079 - val_loss: 0.5945 - val_acc: 0.7742\n",
      "Epoch 196/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4948 - acc: 0.8064 - val_loss: 0.6162 - val_acc: 0.7811\n",
      "Epoch 197/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4942 - acc: 0.8028 - val_loss: 0.6084 - val_acc: 0.7797\n",
      "Epoch 198/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4937 - acc: 0.8076 - val_loss: 0.6129 - val_acc: 0.7797\n",
      "Epoch 199/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4942 - acc: 0.8070 - val_loss: 0.6062 - val_acc: 0.7769\n",
      "Epoch 200/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4932 - acc: 0.8061 - val_loss: 0.6159 - val_acc: 0.7797\n",
      "Epoch 201/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4928 - acc: 0.8076 - val_loss: 0.5963 - val_acc: 0.7742\n",
      "Epoch 202/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4934 - acc: 0.8097 - val_loss: 0.6180 - val_acc: 0.7818\n",
      "Epoch 203/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4943 - acc: 0.8073 - val_loss: 0.6144 - val_acc: 0.7825\n",
      "Epoch 204/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4932 - acc: 0.8073 - val_loss: 0.6036 - val_acc: 0.7756\n",
      "Epoch 205/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4927 - acc: 0.8058 - val_loss: 0.5980 - val_acc: 0.7769\n",
      "Epoch 206/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4920 - acc: 0.8049 - val_loss: 0.6060 - val_acc: 0.7762\n",
      "Epoch 207/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4923 - acc: 0.8058 - val_loss: 0.6062 - val_acc: 0.7769\n",
      "Epoch 208/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4924 - acc: 0.8058 - val_loss: 0.6062 - val_acc: 0.7797\n",
      "Epoch 209/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4921 - acc: 0.8067 - val_loss: 0.6014 - val_acc: 0.7742\n",
      "Epoch 210/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4921 - acc: 0.8058 - val_loss: 0.6056 - val_acc: 0.7742\n",
      "Epoch 211/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4916 - acc: 0.8082 - val_loss: 0.6080 - val_acc: 0.7818\n",
      "Epoch 212/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4920 - acc: 0.8049 - val_loss: 0.5969 - val_acc: 0.7735\n",
      "Epoch 213/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4918 - acc: 0.8076 - val_loss: 0.6069 - val_acc: 0.7825\n",
      "Epoch 214/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4917 - acc: 0.8070 - val_loss: 0.6143 - val_acc: 0.7811\n",
      "Epoch 215/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4910 - acc: 0.8088 - val_loss: 0.6190 - val_acc: 0.7804\n",
      "Epoch 216/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4917 - acc: 0.8082 - val_loss: 0.6033 - val_acc: 0.7790\n",
      "Epoch 217/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4913 - acc: 0.8067 - val_loss: 0.6023 - val_acc: 0.7749\n",
      "Epoch 218/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4908 - acc: 0.8070 - val_loss: 0.5972 - val_acc: 0.7762\n",
      "Epoch 219/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4914 - acc: 0.8073 - val_loss: 0.6009 - val_acc: 0.7735\n",
      "Epoch 220/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4911 - acc: 0.8061 - val_loss: 0.6153 - val_acc: 0.7790\n",
      "Epoch 221/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4904 - acc: 0.8073 - val_loss: 0.5914 - val_acc: 0.7790\n",
      "Epoch 222/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4909 - acc: 0.8076 - val_loss: 0.6044 - val_acc: 0.7769\n",
      "Epoch 223/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4908 - acc: 0.8061 - val_loss: 0.6083 - val_acc: 0.7818\n",
      "Epoch 224/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4894 - acc: 0.8094 - val_loss: 0.5902 - val_acc: 0.7742\n",
      "Epoch 225/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4895 - acc: 0.8076 - val_loss: 0.6239 - val_acc: 0.7825\n",
      "Epoch 226/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4894 - acc: 0.8079 - val_loss: 0.5899 - val_acc: 0.7790\n",
      "Epoch 227/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4907 - acc: 0.8076 - val_loss: 0.5997 - val_acc: 0.7756\n",
      "Epoch 228/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4887 - acc: 0.8076 - val_loss: 0.6202 - val_acc: 0.7811\n",
      "Epoch 229/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4901 - acc: 0.8067 - val_loss: 0.6036 - val_acc: 0.7762\n",
      "Epoch 230/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4893 - acc: 0.8058 - val_loss: 0.6020 - val_acc: 0.7818\n",
      "Epoch 231/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4886 - acc: 0.8085 - val_loss: 0.6111 - val_acc: 0.7790\n",
      "Epoch 232/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4884 - acc: 0.8094 - val_loss: 0.6039 - val_acc: 0.7804\n",
      "Epoch 233/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4897 - acc: 0.8070 - val_loss: 0.6015 - val_acc: 0.7811\n",
      "Epoch 234/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4903 - acc: 0.8067 - val_loss: 0.6009 - val_acc: 0.7762\n",
      "Epoch 235/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4884 - acc: 0.8129 - val_loss: 0.6286 - val_acc: 0.7811\n",
      "Epoch 236/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4885 - acc: 0.8070 - val_loss: 0.5980 - val_acc: 0.7783\n",
      "Epoch 237/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4879 - acc: 0.8061 - val_loss: 0.6006 - val_acc: 0.7811\n",
      "Epoch 238/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4880 - acc: 0.8082 - val_loss: 0.6113 - val_acc: 0.7797\n",
      "Epoch 239/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4891 - acc: 0.8079 - val_loss: 0.6167 - val_acc: 0.7783\n",
      "Epoch 240/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4875 - acc: 0.8073 - val_loss: 0.6120 - val_acc: 0.7790\n",
      "Epoch 241/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4887 - acc: 0.8055 - val_loss: 0.6072 - val_acc: 0.7783\n",
      "Epoch 242/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4883 - acc: 0.8085 - val_loss: 0.5926 - val_acc: 0.7769\n",
      "Epoch 243/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4869 - acc: 0.8091 - val_loss: 0.6098 - val_acc: 0.7838\n",
      "Epoch 244/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4878 - acc: 0.8067 - val_loss: 0.6060 - val_acc: 0.7831\n",
      "Epoch 245/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4877 - acc: 0.8082 - val_loss: 0.5959 - val_acc: 0.7756\n",
      "Epoch 246/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4862 - acc: 0.8052 - val_loss: 0.6163 - val_acc: 0.7769\n",
      "Epoch 247/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4884 - acc: 0.8076 - val_loss: 0.6022 - val_acc: 0.7818\n",
      "Epoch 248/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4867 - acc: 0.8099 - val_loss: 0.6066 - val_acc: 0.7797\n",
      "Epoch 249/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4868 - acc: 0.8085 - val_loss: 0.6013 - val_acc: 0.7756\n",
      "Epoch 250/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4862 - acc: 0.8085 - val_loss: 0.6073 - val_acc: 0.7804\n",
      "Epoch 251/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4851 - acc: 0.8070 - val_loss: 0.6068 - val_acc: 0.7797\n",
      "Epoch 252/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4850 - acc: 0.8085 - val_loss: 0.5963 - val_acc: 0.7776\n",
      "Epoch 253/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4848 - acc: 0.8082 - val_loss: 0.5839 - val_acc: 0.7852\n",
      "Epoch 254/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4865 - acc: 0.8111 - val_loss: 0.6042 - val_acc: 0.7804\n",
      "Epoch 255/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4849 - acc: 0.8099 - val_loss: 0.6147 - val_acc: 0.7811\n",
      "Epoch 256/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4856 - acc: 0.8097 - val_loss: 0.5990 - val_acc: 0.7818\n",
      "Epoch 257/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3378/3378 [==============================] - 0s - loss: 0.4855 - acc: 0.8079 - val_loss: 0.5891 - val_acc: 0.7783\n",
      "Epoch 258/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4860 - acc: 0.8058 - val_loss: 0.5896 - val_acc: 0.7776\n",
      "Epoch 259/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4847 - acc: 0.8102 - val_loss: 0.5982 - val_acc: 0.7811\n",
      "Epoch 260/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4847 - acc: 0.8102 - val_loss: 0.6019 - val_acc: 0.7762\n",
      "Epoch 261/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4845 - acc: 0.8070 - val_loss: 0.6147 - val_acc: 0.7811\n",
      "Epoch 262/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4855 - acc: 0.8108 - val_loss: 0.6022 - val_acc: 0.7776\n",
      "Epoch 263/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4837 - acc: 0.8097 - val_loss: 0.5971 - val_acc: 0.7825\n",
      "Epoch 264/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4847 - acc: 0.8091 - val_loss: 0.6047 - val_acc: 0.7776\n",
      "Epoch 265/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4854 - acc: 0.8105 - val_loss: 0.6001 - val_acc: 0.7831\n",
      "Epoch 266/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4844 - acc: 0.8070 - val_loss: 0.6062 - val_acc: 0.7825\n",
      "Epoch 267/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4841 - acc: 0.8088 - val_loss: 0.5987 - val_acc: 0.7783\n",
      "Epoch 268/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4840 - acc: 0.8120 - val_loss: 0.5997 - val_acc: 0.7831\n",
      "Epoch 269/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4843 - acc: 0.8082 - val_loss: 0.5882 - val_acc: 0.7783\n",
      "Epoch 270/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4845 - acc: 0.8111 - val_loss: 0.5969 - val_acc: 0.7776\n",
      "Epoch 271/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4839 - acc: 0.8091 - val_loss: 0.6125 - val_acc: 0.7811\n",
      "Epoch 272/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4838 - acc: 0.8061 - val_loss: 0.5933 - val_acc: 0.7776\n",
      "Epoch 273/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4845 - acc: 0.8079 - val_loss: 0.5988 - val_acc: 0.7790\n",
      "Epoch 274/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4835 - acc: 0.8094 - val_loss: 0.6112 - val_acc: 0.7811\n",
      "Epoch 275/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4820 - acc: 0.8099 - val_loss: 0.6034 - val_acc: 0.7818\n",
      "Epoch 276/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4836 - acc: 0.8111 - val_loss: 0.5993 - val_acc: 0.7790\n",
      "Epoch 277/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4825 - acc: 0.8097 - val_loss: 0.5921 - val_acc: 0.7818\n",
      "Epoch 278/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4831 - acc: 0.8111 - val_loss: 0.5991 - val_acc: 0.7783\n",
      "Epoch 279/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4841 - acc: 0.8088 - val_loss: 0.6052 - val_acc: 0.7804\n",
      "Epoch 280/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4837 - acc: 0.8111 - val_loss: 0.5929 - val_acc: 0.7762\n",
      "Epoch 281/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4830 - acc: 0.8111 - val_loss: 0.6117 - val_acc: 0.7818\n",
      "Epoch 282/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4823 - acc: 0.8132 - val_loss: 0.6082 - val_acc: 0.7790\n",
      "Epoch 283/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4827 - acc: 0.8102 - val_loss: 0.6067 - val_acc: 0.7776\n",
      "Epoch 284/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4818 - acc: 0.8091 - val_loss: 0.5928 - val_acc: 0.7845\n",
      "Epoch 285/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4815 - acc: 0.8102 - val_loss: 0.6072 - val_acc: 0.7831\n",
      "Epoch 286/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4824 - acc: 0.8102 - val_loss: 0.5994 - val_acc: 0.7783\n",
      "Epoch 287/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4810 - acc: 0.8120 - val_loss: 0.6084 - val_acc: 0.7769\n",
      "Epoch 288/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4817 - acc: 0.8079 - val_loss: 0.6020 - val_acc: 0.7776\n",
      "Epoch 289/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4811 - acc: 0.8085 - val_loss: 0.6049 - val_acc: 0.7790\n",
      "Epoch 290/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4817 - acc: 0.8111 - val_loss: 0.5882 - val_acc: 0.7852\n",
      "Epoch 291/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4828 - acc: 0.8099 - val_loss: 0.5894 - val_acc: 0.7790\n",
      "Epoch 292/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4812 - acc: 0.8105 - val_loss: 0.5988 - val_acc: 0.7804\n",
      "Epoch 293/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4811 - acc: 0.8120 - val_loss: 0.6138 - val_acc: 0.7790\n",
      "Epoch 294/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4808 - acc: 0.8111 - val_loss: 0.6081 - val_acc: 0.7845\n",
      "Epoch 295/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4810 - acc: 0.8079 - val_loss: 0.6010 - val_acc: 0.7845\n",
      "Epoch 296/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4807 - acc: 0.8120 - val_loss: 0.5839 - val_acc: 0.7797\n",
      "Epoch 297/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4824 - acc: 0.8120 - val_loss: 0.5867 - val_acc: 0.7797\n",
      "Epoch 298/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4807 - acc: 0.8108 - val_loss: 0.5985 - val_acc: 0.7825\n",
      "Epoch 299/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4819 - acc: 0.8076 - val_loss: 0.6082 - val_acc: 0.7825\n",
      "Epoch 300/300\n",
      "3378/3378 [==============================] - 0s - loss: 0.4801 - acc: 0.8126 - val_loss: 0.6002 - val_acc: 0.7804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x106e6f080>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\n",
    "batch_size = 10\n",
    "epochs = 300\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[history_cb],verbose=1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 0.7803867403314917\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 0.0946  0.0117  0.0014  0.0552]\n",
      " [ 0.0007  0.0394  0.0076  0.0594]\n",
      " [ 0.0062  0.02    0.0518  0.0318]\n",
      " [ 0.      0.0173  0.0083  0.5946]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "Cm = confusion_matrix(y_eval,pred)\n",
    "C = np.sum(Cm)\n",
    "Cm = Cm/C\n",
    "print('Confusion Matrix:')\n",
    "print(np.array_str(Cm, precision=4, suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fc88198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valAcc = history_cb.val_acc\n",
    "valLoss = history_cb.val_loss\n",
    "epoch_it = np.arange(1,301)\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch_it, valAcc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.tight_layout()\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch_it, valLoss)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VNXZB/Dfk30lQEJCIEAICauGLQKyRkBEETfcsLVF\ni7ji0te2WK1LK63Qaq1LfYsb9nWrVWxFFBQkLoCsgsgmW4CwhbBkg0CW8/5xJ5NJMsvNZGbunbm/\n7+fDh8yZO/c+M8m9z5zlniNKKRAREZlNmNEBEBEROcMERUREpsQERUREpsQERUREpsQERUREpsQE\nRUREpsQERUREpsQERUREpsQERUREphRhdAD+kJKSojIzM50+V1lZifj4+MAG5AFj0sdMMa1fv75E\nKdXB6Dh8LdjOHcCccTEm93SfP0qpkPs3ePBg5cry5ctdPmcUxqSPmWICsE6Z4G/d1/+C7dxRypxx\nMSb39J4/bOIjIiJTYoIiIiJTYoIiIiJTYoIiIiJTYoIiIiJTMv0wcxGJB/B3AOcAFCil3jI4JCIi\nCgBDalAi8pqIFIvID03KJ4rIDhHZJSKzbMXXAHhfKXUbgCsCHiwRERnCqCa++QAmOhaISDiAFwFc\nCqAvgKki0hdABoADts1qAxhjI1XVtdh+pAw7j5Z73G7JliPYc6wCZVXVLTqGUgpbD5XZH287XAbt\nlgHPth8pQ21dw7alp6tx8NSZRtvsO16J8qpqbD9S1qi86XEdnT5Xg8KSykZllWdrsO+4VlZcVoVj\n5WfdxrbnWAW2HipDVbX7X59SCtsOl6H0TDWKTp5u9FxtnUJReR1KKs6iuKwKgPb51NW5/3wqztbg\n1W/2ovRM49+FUgoLNx3C2RrD/qSCilIKB8rrjA6DLMaQJj6l1FciktmkeAiAXUqpPQAgIu8CuBJA\nEbQktRFuEqqIzAAwAwDS0tJQUFDgdLuKigqXz7nz/HdVWH9Uu5g9e1Es2kY7D+X3q85gT6l2IqfF\nCeaMjvO47/qYVh6qwbzvz2LmwGjERQjmrK3CT/pE4eJukW5ff6C8Dr9bcQZX9ojE1TlRAIB7llWi\nohqYP7HhzvFpixsSzaPDYpDVNhwAsOJgNV7efA73DYrGwNSIRjHNWXMG207UNdpP/XucPzHevk/H\n5x2drVW4/XMt2VzQMRx3D4hx+T4KDlRj/pZz9seO+3z/x3P4eE81HlmxFADw8NAYzF5dhRt6ReHS\n7q4/n/r4/vDx1kb7W7y3Gu/uOIf4SODFcea4u97M3vx2H3634gx69CnB8OwUo8MhizBTH1RnNNSU\nAC0xDQXwHIAXRGQSgIWuXqyUmgdgHgDk5eWp/Px8p9sVFBTA1XPuzCxYYv/5/EFD0KNDgtPtpi1e\nZP/56Gml61j1Ma1ZvB3AbsSkZqJ9QhSwdjOq41ORn9/f7euX7ygGVqzFqfC2yM8fAgCosMXR6PgO\nsaVl9UV+bjoAYOUn2wDsQWxad+SP6dEopmlO9tOozNlxHJSergY+/wwAsLs83O3nsew/PwDYZ3/s\nuO28nd8COG5/nNq9N7B6I87EpCA/f6DLfTq+Z8f9vXdwPYAjqKx2HTs12HpYaznYU1LJBEUBY6YE\n5ZRSqhLALf4+zksFu7Gu8ARenXZBs+c+2nQI5VU19sfjnv4Sd+b3wL7jlchJTcQDF/fEuKcLsPtY\nZbPXNvXEwi14fUUhAGBiv45YvOWI9oTDhfTPS3bYf35vXRHeW1dkf7zliUvQ7zEtWW79/SX4YMNB\n/O4/Wlfelz8eQ+asRfjgzgvt22fOWoTk+Cgcr2yomTg+5+ipT7fjqU+3NxQ4xFS/7abHJjh9feas\nReiT3gbbDmtNhTcN7Yq3V+9vtP8TleeaHRMAhmS2x5rCE07ju21Ud7z89d5mz9337kYA2u/mo02H\ntLJxOXjg4p6Y9voaFOw4hkv6pbl9v/Vc/e4+njkS53VOcvoaq5nQLw3vrNmPXh0TjQ6FLMRMw8wP\nAuji8DjDVhYQcxZvx7LtxU6fu//d75qVvVSwG59sPoK/LdsJALqSEwB7cgLQkJxaYNOBU/afdxwp\ntycnR3/9fGejx86Sk7e++vGYy+fqkxOAZsnJHWfJqZ6z5ORK/e+iYIcW45ItR3W9ztXv7qWC3bqP\nHeqiwrVLhc4uUSKfMFMNai2AHBHpDi0x3QjgJl8eYHNRKcrOKrz57T7kZiShurYOsZERWPj9Ifs2\nJRVn8dCCzfh8q3Zx+/O1ufDQD+/ymzkAXDB7Kf5w5Xm44831PnkPN72y2v7zDfO+dbrNN7tKPO7n\n7rc3eHX8me80T9Zmcv7jSzxvpNOizYfxos/2FtzCRAAANXUcKEGBY0iCEpF3AOQDSBGRIgCPKaVe\nFZF7ACwBEA7gNaXUFl8ed/IL39h+al7rqJf35NJGj3/1/vetOuax8rM+S05NnavhxaIpx6ZYXzh4\n6gw6t4316T6DUUS4lqBqPX1bI/Iho0bxTXVR/gmATwIcDpFLZ875NuEFq93FFQCAm19dg8KnJhkc\nDVmFmfqgiEyHFQbNrAWbjQ6BLCikEpSITBaReaWlpUaHQiGipML9TchE5D8hlaCUUguVUjOSkjg0\nmHxDIEaHYAoT+qZ53ojIx0IqQRH5mgLb+ACgW7LnGVGspKyqWvc0ZOQ9Jigi8sjVzClWdODEaeQ+\n/hneWFlodCghjwmKiDw6P4PN5vUKbRMlL93m/MZ+8h0mKCLyKDG6YULePccqDIwkOFVV1+KKF77B\nRoeZYMgzJigi8qirQx/U2Ke/xI9Hy/HJ5sMGRhRcthwqw/dFpXhioU/nHgiYHw6WInPWooB/OWGC\nIqIWm/DXr3DXW95Nl0XB5z/fadOiLgtwsyYTFBERmVJIJSjeqEu+xvug3OOKxORPIZWgeKMu+Vp1\nLSfkdWeqw4z6mbMW4YF/bTQwGgo1IZWgiHzt5GnfraUVijbs10al7T9+GgDw4XcBW8KNTOKjTYeQ\nOWsRisurfL5vJigiN0TYxOfJP1cVYvSflxsdRlAI1skn3IX99up9AIBdxb4f4ccEReQG05Nnj/5X\n39Dp4rIqPL9sZ7Mpgu5+e4NPF5o0o2D8nlNXp3DjvFX40mEV7UC/DyYoIjeC8cJiVjPf+Q5Pf/4j\nthwqQ12dwtzF21FcXoVF3x/2+UKToe61b/ZizuLtfj1GeVUNvt1zAjO9XH3bF5igiNwIY4Zqlbo6\nhdmLtuJw6RmcqdZG/M1dsgPLthfj7wW78WsdK1av3F2Cf3y52+lz1bV1WHHQvxO3rtl7Aicrz2HF\nrhKUV1X77Tgt8fuPt+KlAuefSSgxZEVdomDB9NQ6G/afxMtf78Xmgw23fnz14zHst81nd67G+ShJ\npRSeWLgVUwZl4KaXVwMAbh/To9l2//hyN17efA79Nh7CVQM7ex3ngROnIQKEhwlmL9qGv1zXHzGR\n4VBK4fp/rELHNjE4UlaFMT07YHL/Ti3efyD7nt5Zsx+ZyfG4sEeyT/ZnZLcZExSRG4kxkZ43Ipfq\nVySubbI0cVW1lphW7j7u9HXlZ2swf2Uh5nuYMfxYubag5Kkmoy3X7zuBgV3aISxM31eMUXO1QR6T\nctOx6PvDuKRfx0aJ6EiZNkJtV3EFHvz3Jl37dEZPhbzo5Gl8sb0YP7sw06tjPGRb/bjwqUlevd7O\nIVajBnewiY/IjeSEKKNDCDpKKZyrqcPawhPYcbS8xa//eucxFJc1H7J8tqYWX/14DB9/f8jj66e8\ntAqvfLPH6fPr951A5qxFOFLa8mHRgVgDauSc5Xj0v1twstI3tzjMX7EXD3+oJa2Vu0qQOWsR3rKN\nvNPF4S0/uWgbDpee8UlcerAGReRGsHdBiUg8gL8DOAegQCn1lr+POWruchSdbHwRq1PA90XuZ3jZ\nVVyB6to63PzqGsRHhTd7/uWv9uAvn/0IALg8txMyZy1yup9Dp87Y91fvWPlZvL5iLx6c0Av/XKVd\nnL/dc7xZs2B9jcxfWpLf6rxIhuc/1nw05OMLtwIALp4Yj9dWFAIAHv7wB4zvk4a0NjEu9+Xqb//h\nD3/Aa9MuaHFs3gipGhSnOiJfM+NURyLymogUi8gPTconisgOEdklIrNsxdcAeF8pdRuAKwIRX9Pk\nBAA7jjSuSTlbqXj8M1/i0r99DQCoPNd8CiVnZXo9tGAz/l6wGyt2l7jdznE5jJKK5snqkBe1LqDx\nxX5vSSWe+WyHX2pj5Wddj4bcerwW+09U2h//8j19s340jTKQZ0RIJShOdUS+lhhjykaG+QAmOhaI\nSDiAFwFcCqAvgKki0hdABoADts1MM3He0bKW11Racz2vnzPQsSusqroWlS4u6Mt3FCPvyaUo2HHM\n6fP1Mmctwu3/t65FsUx7fQ2e+2IXDnuZ7PRq+t7mrq3Cj0cbapUrdjnv/6tnhq9mpjz7iMg1pdRX\nIpLZpHgIgF1KqT0AICLvArgSQBG0JLURLr6QisgMADMAIC0tDQUFBT6PucLNN3u91u8otP/cNMZX\nlm9DZrXWdLe9SBsKfvjwERQUnAQAnDyhJYPvN23C0aNaLLMWbMasBZsxf2K8fT91dbbBG9u1fq6/\nLFzvMp6TJ08AAJZsOdosnoqKikZlu05pCfLI8VKcsw1cXLVqFZJjXdcRVqxciTZRgqLyOsRHAu1i\nmm/r7nd1y0tLcdcA1014APDEm59j76k6TDsvutlzZ2q0bF5TU4OiogP28pLjx+3HfW5DFTYUa+9t\n08ZNOHegedNsazBBEYWGzmioKQFaYhoK4DkAL4jIJAALnb1QKTUPwDwAyMvLU/n5+c6PsNh5n0+g\nrD3SUAH88EgSgIbmqqIKhTZZ/TGoazscXbsf+GEz0tM7Ij+/PwDg1d2rgeMlWHo0BpsON+4CGD16\nDLD4EwBAWFgYUFeHuLg44HQlthx3PVmw43NNP7OCgoJGZW32nwS+XYkjpxXSk2KAqioMu/BCdG4b\n23zHts95xPDhSE6Itve1NRqVZ9smvfdg1NTVoV+nJHy98xiANfZNyhCH/PzRbn9vr/+gDcSYf88l\nzZ6rOFsDLF2C8PBwfLav4QtGSnIy8vO1PqhpDvvuP6A/hvdIcXksbzBBEbmRYM4mPt2UUpUAbjE6\nDl/778bmI/ne/HYfBnVt5/Z1m5wM1LjzLYdako+7heqTyzu3DbOXtabpbG3hCRyvaBjdd8mzXwHQ\nktfNr65x9bIW7f8/3x3E7KvPt5dV1xl3J1RI9UER+dI7tw1Dm+C5D+oggC4OjzNsZZa1fMcx+zLl\n7pZNWbLlaKuPVVVdi1+/vwklFWfx7pr92Fhcg6rqhhpf0/u0vLGu8ASu+99VuOPN5s2Ozm54rh/N\n2BLX/e8qvLV6PwCHRGrgnbrB/fWQyI98dSd+gKwFkCMi3aElphsB3GRsSIG1YMNBPHP9APvj+iHj\nY5/+ssX7auk1+d53vsNnW4+ipk5hwQbte8GnB1e4P4aHUR+7j1UiOaGhb+gJ23BxZ5zdPFzmo/kN\nzxm4JhprUERBRkTeAbAKQC8RKRKRXyilagDcA2AJgG0A3lNK6ZtmPITU1SkcPOXf0XHOfLZVq4Wt\n2NUwjN3VTcp6l3BZt+9Eo8eO00U19dEm5zcvb/Zw75k7ZrgHkDUosqQb8rrgX+sOeN7QhJRSU12U\nfwLgkwCHYypZvzX27bdk+PzIOcvx/NSBLuf2q6lVWLnL/X1bnpw+5/tZ4n84VIrS09UoPeP/iXNZ\ng6JG3p4+FB/ceWGzcscRRHrn+OrfpW2LX9PUE1f0Q1aHeM8burDp0QlOy+dcm9v6ucoo5Di7gdgX\n7nyrYcmKMw59U4995LqS+8znP+KmV1a36rh6382db67H8h3Fjcpc3aR+tOws+v/+s2aLVNbUKrxU\nsNt+z5kvsAZFzXgaCaWXL1oIRGDsdMpkKdW1/v9jc5zC6ETlOSil7M1+vrhfzJHem5s//eEIjjvM\n/bfzaDky2sW16Fhvr96PxVuOoLq2DveOy2nRa10JqRoUpzryDS5zbj08dxrbW1LpeSMvNZ3h4a3V\n++0DJl4q2OXTY7WkRrhmb0Of18nT1S3ugzptqxlWnqvBwVNncKYVU1PVC6kEZfapjqYMytC1XVZK\nPPK6tUNWh3g8MqkPnr6uv9vtx/dJc1r+yKQ+AIDeHRNx4wVdnG7jyoRuDZXrCX21/f90WFfMvTbX\n5WuyUxPwR4f7J87r3AYAcNPQri06tiOlgL9c3/D+L8hsqN2lJkYjOzXB6etiI7U72p1NVZSb0fD3\n4bg/R49P7utVvMHK7OdOKGlaS3vkPz+g+0OfIHPWIry43MeLEHpZIWztPIEjnvoCt8xv/X1ZbOJr\ngRdvGoS7W7H88eieKfhgQxHaxES4HQK64K7haBvXeJmHtDYx+OmrWnv0qJwUfL2zBL07JmLx/aMB\nNNwQ2DYuEqdOa52X00dlYfqoLPs+3l3relDAsKz2+HbPCXu73E19ojHvzsZ3lz951flOXqlx7M/5\nrW1q/+iIcKf9PIVPTXI5E/W9Y7Px3BeNv0U6Njk+dFkfXPP3lejfpS3+e/cI/OnTbdhVXIGslHjs\nsX3r9dS3NDm3oVP633cMxxfbj+LW+Y3nU5s2orvbfRAFA2/TTE2dwlkXi0m6UmFbbbjQdh5+u+eE\nu811YYIKoPovJa1tQgvpJrgm763pW7V/hvYCNClo8SGIQtZPvBxk4c3rNuzXZoL3xY3P9UKqiS9Y\neHOBtMpFVecCqPbPg+MniEIXE5QONw/rhsxkzyNa/ufinuiWHIcrB3TC5bnpGNc7tdHzbWIjkBQb\niccm98XPL+zWohjO69TQNzBlUOdmzz82uS8y2sXi8cn9AABDMts32+bhy/o0+h/Q+nFmX30eZo7N\nQWS4oF96y/sgHr3ceX9N01zz4ISe6JWW6HZfAsHto7OQEK1V7sfaPsMR2ckY3zUCPdMSEBURhvts\no4SuHaz16z11jdY3NnVI8/6utDbNZ2puekxHD4zv6XZ7IgoMNvGhoc+ivl9k7rW5uD6vi/3x9FHd\n8YerzsOi7w+7fT0AzHQyvPLmV1fj650lCA8Lw6bHtPtyrh6YgSeuPA8AMPZPn2JPaUN7r7P7D5Li\nIu3H2XqorNnzt4zojlts/SZNVwmtd9voLNw2WuuTmv3JNgDAmofH25/fOfsyp69z5tYR3fHair3a\nzyMb99f89rLe+OMn25u95p6xObhnbOPPJzUxGsW2KWnSk2IwZXBnZLSLw0MOSRQA3po+DAUFBUiM\nicSPT15qL++Zlmj/XFz1Pa3+7XgcLavC0D8uc/uexvTsgDduHeJ2GyIKHNagyCuPuhnl1tDX5nk/\nnz0w2v7zqofGtfjeC73chmKR5lOiYMMERX6jZzCHH1a9JqIQwQTlRGZy46l16vtDurTXFhdrG9ew\nBEN9mTt53bT+oPQk56tb9m7feBXKqAj3v5b28doQ9GFZ5pxtu6etn6lPuvv+pnpZKd5PZeQLnZK0\n3+Hgbr6ZQYOIfMPyfVDObsgc0l1LKBsfvRjbDpfbp7zPzWiLpb8cg+4p8Sg8Xom4qHB78nJn5ths\nXN4/HT06OL+pdEpOJH4ybhD6dWqDsjM1iI1yv2xyx6QYLH8wHxntPCdHI1zUOxVLfzna5fsFgJjI\nMFRVa/1uH80cifIq/0886UqvjolY9j9j0D3Z2ERpJBGZDGBydna20aEQ2Vm+BtXNzbf3tnFRzdYE\nyk5NQHiYoEeHBKQnxSJRx4J2YbbtXQkPE4zITkHbuCh01TFaEAC6p8QjMty8v77s1ES3TXwxkQ1J\nOCE6AulJfk62Hlobe3RIQJjeMe4hiDNJkBmZ9wpHRESWxgRFluBq6QAiMq+Q6oNqaTt6amJ0o3ne\nbh+dZV8mmjy7akAnpLf1rmluzpRczFm83elkrv7QLi4SItqowUm56QE5JhG1TkglKKXUQgAL8/Ly\nbvO0bXpSDFY9NK5RWdObQ8m9Z28c6PVrL+nXEZf06+jDaNyLCA/D3j9xgUKiYMImPiIiMiUmKCIi\nMiUmKCIiMqWQ6oPSa2K/jrh9TJbnDUPYLSMy7TNcEBGZkaUSVHiY4LLMCDx/82CjQzHcY7ZlOYiI\nzIpNfEQEEZksIvNKS0uNDoXIjgmKiDjVEZkSExQREZkSExQREZkSExQREZmSpRKU4vKtRERBw1IJ\nCoDHdYGIiMgcrJegiIgoKDBBEZEuUbxaUIDxT46IdGkbw/ZxCqyQSlC8G56IKHSEVILi3fBE/sNB\nsBRoIZWgPOH5RUQUPCyVoACOMidyRk/zeEai5S4XZDD+xRGRrubxxCh+vaPAYoIiIiJTYoIiIiJT\nYoIiIiJTslSC4jBZIu8NTgs3OgSyGEslKICj+Ii81b9DhNEhkMVYLkEREVFwYIIiIiJTYoIiIiJT\nYoIiIiJTYoIiIiJTYoIiIiJT8pigRGSmiLQLRDBERET19NSg0gCsFZH3RGSiiPBWIiIi8juPCUop\n9QiAHACvApgGYKeI/FFEevg5NiIKEK5GTWakqw9KKaUAHLH9qwHQDsD7IjLXj7ERUYBwNWoyI49z\nl4jIfQB+BqAEwCsAfqWUqhaRMAA7AfzavyESEZEV6Zlcqz2Aa5RS+xwLlVJ1InK5f8IiIiKr09PE\n9ymAE/UPRKSNiAwFAKXUNn8F5g137eiKU5kTEQUVPQnqJQAVDo8rbGWmo6cdnWMQiYiCg54EJcqh\n+qGUqoO+pkEiIiKv6UlQe0TkXhGJtP27D8AefwdGRETWpidB3QFgOICDAIoADAUww59BEREReWyq\nU0oVA7gxALEQERHZ6bkPKgbALwD0AxBTX66UutWPcfkcB/GR2dhmYylSSp0VkXwAuQD+qZQ6ZWxk\nrt02qjte/nqv0WGQRehp4vs/AB0BXALgSwAZAMr9GZQ/cRAfmcgHAGpFJBvAPABdALxtbEju8Yse\nBZKeBJWtlPodgEql1BsAJkHrhyKi1qlTStUAuBrA80qpXwFINzgmItPQk6Cqbf+fEpHzACQBSPVf\nSESWUS0iUwH8HMDHtrJIA+PxqHd6G6NDIAvRk6Dm2daDegTARwC2Apjj16iIrOEWABcCmK2U2isi\n3aE1qZvWlEGdjQ6BLMTtIAnbhLBlSqmTAL4CkBWQqIgsQCm1FcC9AGD7EpiolDL1lz8uB0eB5LYG\nZZs1grOVE/mBiBTY5rZsD2ADgJdF5Bmj4yIyCz1NfEtF5EER6SIi7ev/+T0yH+PgIzKhJKVUGYBr\noA0vHwpgvMExEZmGnjn1brD9f7dDmQKb+4haK0JE0gFcD+Bho4MhMhs9M0l0D0QgRBb0ewBLAKxQ\nSq0VkSxoi4AGnIhMBjA5OzvbiMMTOaVnJomfOStXSv3T9+EQWYdS6t8A/u3weA+AKQbFshDAwry8\nvNs8bfvxzJG4/PlvAhAVWZ2eJr4LHH6OATAOWocuExRRK4hIBoDnAYywFX0N4D6lVJFxUXnWuW2s\n0SGQRehp4pvp+FhE2gJ4128REVnH69CmNrrO9vintrKLDYuIyET0jOJrqhJA0PVLccl3MqEOSqnX\nlVI1tn/zAXQwOigis9DTB7UQDaO0wwD0BfCeP4PyJ95nSCZyXER+CuAd2+OpAI4bGA+Rqejpg/qL\nw881APaZvY2cKEjcCq0P6q/QvgSuBDDNyICIzERPgtoP4LBSqgoARCRWRDKVUoV+jYwoxCml9gG4\nwrFMRO4H8KwxEekTHs5mCAoMPX1Q/wZQ5/C4Fg5DY4nIp35pdACetImJxLThmUaHQRagJ0FFKKXO\n1T+w/Rzlv5CILC0oqicTz+todAhkAXoS1DERsTdDiMiVAEr8FxKRpQXFcNOgyKIU9PT0Qd0B4C0R\necH2uAiA09klzCwoznqyBBEph/M/SQHAu2CJbPTcqLsbwDARSbA9rvB7VEQhTCmVaHQMrcUvfBQI\nHpv4ROSPItJWKVWhlKoQkXYi8mQggiMiIuvS0wd1qVLqVP0D2+q6l/kvJCIiIn0JKlxEousfiEgs\ngGg32xMREbWankESbwFYJiKvQ+vEnQbgDX8G5S2uaUNEFDo81qCUUnMAPAmgD4Be0BZY6+bnuLyi\nlFqolJqRlJRkdChEIS0ijAPNyf/0zmZ+FNrAnesAjAWwzW8R+QknMyfyncHd2hkdAlmAywQlIj1F\n5DER2Q5tQsv9AEQpdZFS6gVXrzMrZRsYy0RF1Hoigplj9TWlR3LuPvKSuxrUdmi1pcuVUiOVUs9D\nm4cvKJWergYAfHGgxuBIiELXv2YMa1YWF6Wnq5uoOXcJ6hoAhwEsF5GXRWQcOMMJEbkxNCvZ6BAo\nhLhMUEqp/yilbgTQG8ByAPcDSBWRl0RkQqACJCJzGtqdyYj8S88ovkql1NtKqckAMgB8B+A3fo+M\niExtZE6K0SFQiNM7ig+ANouEUmqeUmqcvwLyF46NIHJNRCaLyLzS0tIWve71Wy7AgruG+ykqsroW\nJahQwE40oua8vYfwol6pGNTV/ZBz4UlHXrJcgiKiwLp5mCnv66cgwARFRK32yKQ+6J4S7/S5/hlt\nAxwNhQreoEBErTZ9VBamj8oyOgwKMaxBEZFfsQ+KvMUERUREpmSZBMU5+IiIgotlEhQRGYNNfOQt\nJigi8qvB3dobHQIFKSYoIvKb5PgoJMVGGh0GBSkOMyciv/jgzgvRpV2c0WFQEGOCIiK/YNMetRab\n+IiIyJQsk6AU5zMnIgoqlklQ9TjilSjwYiPDjQ6BgpDlEhQR+deCu4bjo3tGNCpb98h4g6KhYMYE\nRUQ+NahrO+Q2mcE8Ptr1eKz+XTjbOTnHBEVEhpqhYxb0zY9PCEAkZDZMUERkmG7JcZiUm46XfjLI\n7XaJMZH4/IHReG1aXoAiIzOwzH1QnCyWyHzqBy0lxHi+FOWkJaLW4UTe9OgERIQL+j22xE/RkdEs\nV4PixJVE5hFjG903MjsFT1/Xv0WvTYqLdNu3VS89Kcar2Mh4lktQRGQOD07oiVd+rjXZiQimDM7w\ny3E6JEb7Zb/kf0xQRGSIe8bmIMPNXH3jeqcGMBoyIyYoIiIyJSYoIiIXMpM5G7uRmKCIiFzI78Vm\nRiNZJkF8e9DNAAAOP0lEQVRxlDkRUXCxTIIiotDUJor3jrTWvJsHGx2CU0xQRBTUctq5v4z165QU\noEiCV2S4OVOBOaMiopA1pLu+lXYVgEcm9UHf9DatOt7jV/T1+rVXDOjUqmNT61hmqiMiMod/3joE\np8/V6tp2+qgsTNcxmaw70RHer0U1qGs73JDXBf9ad6BVMZB3WIMiooCKiQxH+/goj9uZpWfpjvwe\nRofgf2b5sJuwTIJSnC2WKOi1i/Oc2HzNV6sB56Qm+GQ/VmKZBFXPpF8UiAwlIpNFZF5paanRobiV\n1qb5xK++/u7528t6N3rc0UeTzY7tnYprBnX2yb6swnIJioiaU0otVErNSEqy7oi3bFsNJ8pPI9p8\nmUd7pvm2NmbWL+5MUEQUUlo7e7kYtCbP3Cm5urft3DbWj5GYBxMUEQW1pjUTb9OL0f3U11/QxdDj\nmxETFBFZxjUDPfcBmW1R09bUlvpn6GuyNarW6AnvgyKigHjpJ4NQdPJMq/ez6dEJUF726DxzwwAs\n+O6g0+fMepG2MsskKI4yJzLWpeen+2Q/SXGRPtlPamI0isvP2h/XD46ICPNPw1KX9nEoqTjreUOy\nM30Tn4hkicirIvK+0bEQkfnNGO185omrPExbdP/4HNyZ3wPX+njp+c2PT8Db04fip0O7tuh11+f5\nIA6dtUKz1h39mqBE5DURKRaRH5qUTxSRHSKyS0RmuduHUmqPUuoX/oyTiELHWCdLxRc+NQnP3jiw\nUVnTa3dCdAR+M7E3oiJ8e1lMjInE8OyUFjUhrntkPGaOzWn1saPC9R9zwV3D8dE9I1p9TF/ydw1q\nPoCJjgUiEg7gRQCXAugLYKqI9BWR80Xk4yb/uFoYEbnVIdb1RfiDO4e7fe26R8ajT/1ktG6u5V//\n+iIs/eVob8IzzKicFIzO6aB7+0Fd2yE3o22rj+vLGTP82gellPpKRDKbFA8BsEsptQcARORdAFcq\npf4E4HJ/xkNEoee6XlG4fswAPPDeRpRX1diHT3RtH4fB3dq5fW1KQjSSYj1fBru0d730+ws3DURt\nncJ9725sSdi6tWbsRh+dM8H7cnxIbJRvpoYCjBkk0RmA49TARQCGutpYRJIBzAYwUEQesiUyZ9vN\nADADANLS0lBQUNDo+WOn6wAAStU1e85oFRUVjEkHM8ZExosME+T3TcOHd43Alz8eQ0SY/3tUvvrV\nRRj95+UAgMtztb4tfyUoX7lvUDT+tsH7QRr/mjEMN8z71ocReWb6UXxKqeMA7tCx3TwA8wAgLy9P\n5efnN3r+wInTwFfLIRKGps8ZraCggDHpYMaYyDyyUxOQnZqAfccrW/S6hGjtMtiS0Xtdk13XqNy5\nbnAXLNjgfJi7vw1MjQDgOUE9MqkPhmUl4/Lnv2lUPjQrWddxfDli2ohRfAcBON4ynWErIyIKuDlT\ncvHghJ64INN9c6AvXNgjGXv+eBkW3z/Kp/t99HLvF2UEAHHogJs+KgvndTbHnIxGJKi1AHJEpLuI\nRAG4EcBHgTo478UjIkfJCdG4Z2xOwG7UDQsT9O7ovm9I0LAMu7MZ3Ju6dWR3p+XdvKzpAcDArq0f\nMNFa/h5m/g6AVQB6iUiRiPxCKVUD4B4ASwBsA/CeUmqLP+MgouCT30v/CLSW+NuNA7x+7Yd3Dcfv\nWllbcSfcof+sY1IM/npDf8y7ebDX+8tJS/T6tYO7+r9G6Ym/R/FNdVH+CYBP/HlsIgpuPx3WzS/7\nHdpdX1+KMwO7tsNAP164k2IjcaLynP3x1QN9e9OwK76sPHo7DZUzpp9JgoisJz4qnHPjtdCie0fi\n3rHZPtvf7WOMX+qeCYqIKAh9/kDjG4f7dUrCdIdpnu4d13wmCleDM5zNmO64rtb7d1yoK6alvxyj\nazu9QipBuVu2mpPFEpG/+XqaJHc89S9dkNm+WZmzwRlrHx6PzJR4t/vKc7IvZ7IdZpF4fHLr++pC\nKkHpWbaajQZE5ndhD+/7idKTYpGTmoA/XHWe0+fjo7WZDsb2TvP6GPWmDGroI3rlZ3lY1soaRHYH\n7QIf4WLZ+U5JMbg+LwOzrz5f9z4/vW8Uvv71RS6fb+0KxE2lJ2m1sRHZKa3el+lv1CUi63h3xjB8\nX3QKNw/L9HofURFh+NxNokiMicSqh8YiJcH1hfmd24YhJSHK7XF2zr4U4Q79ZOP7tj7hvfyzPGws\nOoWkWOdLikSEh2Hutf1btM/66Y52exlTpyTPw9wdPX19fyzfXtyqEYT1mKCIyDSGZSVjmM4ZC1qj\n/lu+K3pqcJEuajmtkRQXiTE9Wz+8PjHaN5f2r351UaNk+c9bh2D9vpP427KdLl/TJiYSVw7wvHKx\nHiHVxEdEFMqaDmw8v3MS5k7JbdE+sjq4729y1DU5rtECkaN7dsADF/ds0fFagwmKiChILZw5Etdf\n0DBzXLRtkMakXNerF793u74ReWbABEVEZIBhWfpGxjnyNBo5OiIcG353MZ50MUAE0JYY6ZnmuzWb\nHpyg1aiuGeSbZj1HlklQvry7mYjIk/xeHfDNb1yPnnt7+jDd+xrSXX8yax8f5XIUoK89e8MAdPTQ\nn9caIZWg3N0HRUQUSDER4cho53qy1rAWrFv152tb1s/kifjohpurBvq+1uQopBKUnvugiIgCYfoo\n5zOMk34hlaCIiMxC7+wLAHAbk5lTTFBERAZ7eJL/lvAIZkxQREQmFxel3Xibm+Hb7ou/3tCyWSmc\n6ddJm6niol6prd5XU5xJgojI5DokRuOje0agpw+mD3JUPw1Sa/ex7fcTERsV7oOIGrNMguJs5kQU\nzHIzfL8Eu6+ui/5IToAFm/i4BhoRWV2wXActl6CIiPzpjVuHYO5o/928agY3De0akOMwQRER+dCY\nnh2QGhfal9bZV52HnbMv9ftxLNMHRUREviEiiAz3fzthSKV5TnVERBQ6QipBuZvqiIP4iIgaM/vo\n5pBKUHoEyeAVIiLLs1yCIiKi4MAERUREpsQERURkoPoVaak5DjMnIjJI4VOTjA7B1FiDIiIiU7JM\ngkprEw0AmJQVaXAkRETGumNMDwBARntzT8lkmSa+uKgIFD41CQUFBUaHQkRkqKsGdsZVAzsbHYZH\nlqlBERFRcAmpBMWpjoiIQkdIJSh3Ux0REVFwCakERUREoYMJioiITIkJioiITIkJioiITIkJioiI\nTIkJioiITEmU2ZdU9IKIHAOwz8XTKQBKAhiOHoxJHzPF1E0p1cHoIHwtCM8dwJxxMSb3dJ0/IZmg\n3BGRdUqpPKPjcMSY9DFjTFZi1s/fjHExJt9gEx8REZkSExQREZmSFRPUPKMDcIIx6WPGmKzErJ+/\nGeNiTD5guT4oIiIKDlasQRERURBggiIiIlOyTIISkYkiskNEdonILINi6CIiy0Vkq4hsEZH7bOWP\ni8hBEdlo+3dZgOMqFJHNtmOvs5W1F5HPRWSn7f92AYynl8NnsVFEykTkfqM/Jyvj+eM2Lp4/fmKJ\nPigRCQfwI4CLARQBWAtgqlJqa4DjSAeQrpTaICKJANYDuArA9QAqlFJ/CWQ8DnEVAshTSpU4lM0F\ncEIp9ZTtgtROKfUbA2ILB3AQwFAAt8DAz8mqeP54jKsQPH/8wio1qCEAdiml9iilzgF4F8CVgQ5C\nKXVYKbXB9nM5gG0AOgc6Dp2uBPCG7ec3oF0IjDAOwG6llKvZDcj/eP60HM8fH7BKguoM4IDD4yIY\n/IctIpkABgJYbSuaKSLfi8hrgWwOsFEAlorIehGZYStLU0odtv18BEBagGOqdyOAdxweG/k5WRXP\nH/d4/viJVRKUqYhIAoAPANyvlCoD8BKALAADABwG8HSAQxqplBoA4FIAd4vIaMcnldYOHPC2YBGJ\nAnAFgH/bioz+nMgEeP7oEwrnj1US1EEAXRweZ9jKAk5EIqGdXG8ppRYAgFLqqFKqVilVB+BlaE0q\nAaOUOmj7vxjAh7bjH7W1+de3/RcHMiabSwFsUEodtcVn6OdkYTx/3OD54z9WSVBrAeSISHfbt4ob\nAXwU6CBERAC8CmCbUuoZh/J0h82uBvBDAGOKt3U4Q0TiAUywHf8jAD+3bfZzAP8NVEwOpsKhecLI\nz8nieP64jonnjx9ZYhQfANiGVD4LIBzAa0qp2QbEMBLA1wA2A6izFf8W2h/SAGjNAIUAbndov/Z3\nTFnQvvUBQASAt5VSs0UkGcB7ALpCW37heqXUiUDEZIsrHsB+AFlKqVJb2f/BoM/J6nj+uIyJ548f\nWSZBERFRcLFKEx8REQUZJigiIjIlJigiIjIlJigiIjIlJigiIjIlJqgQIiK1TWYx9tms0yKSKSKm\nv2+CyFs8f8wnwugAyKfO2KZcIaKW4/ljMqxBWYBtvZq5tjVr1ohItq08U0S+sE0euUxEutrK00Tk\nQxHZZPs33LarcBF5WbS1eD4TkVjD3hRRgPD8MQ4TVGiJbdJEcYPDc6VKqfMBvABtRgAAeB7AG0qp\nXABvAXjOVv4cgC+VUv0BDAKwxVaeA+BFpVQ/AKcATPHz+yEKJJ4/JsOZJEKIiFQopRKclBcCGKuU\n2mObbPOIUipZREqgLQBXbSs/rJRKEZFjADKUUmcd9pEJ4HOlVI7t8W8ARCqlnvT/OyPyP54/5sMa\nlHUoFz+3xFmHn2vBPkyyDp4/BmCCso4bHP5fZft5JbSZqQHgJ9Am4gSAZQDuBLQlo0UkKVBBEpkU\nzx8DMIOHllgR2ejweLFSqn6obDsR+R7at7iptrKZAF4XkV8BOAbgFlv5fQDmicgvoH3TuxPaAmdE\noYznj8mwD8oCbG3oeUqpEqNjIQo2PH+MwyY+IiIyJdagiIjIlFiDIiIiU2KCIiIiU2KCIiIiU2KC\nIiIiU2KCIiIiU/p/Tgf18KG3HhYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12367b1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ls = history_cb.losses\n",
    "Acc = history_cb.accs\n",
    "ntr = np.shape(x_train)[0]\n",
    "epochNum = []\n",
    "for i in range(31800):\n",
    "    epochNum.append(i*10/ntr)\n",
    "\n",
    "plt.subplot(121)\n",
    "matplotlib.pyplot.semilogy(epochNum, Acc)\n",
    "# plt.plot(epochNum, Acc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(122)\n",
    "matplotlib.pyplot.semilogy(epochNum, Ls)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# plt.axis([0.3,3,0.96,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.58\n",
      "0 1 0.07\n",
      "0 2 0.01\n",
      "0 3 0.34\n",
      "---\n",
      "1 0 0.01\n",
      "1 1 0.37\n",
      "1 2 0.07\n",
      "1 3 0.55\n",
      "---\n",
      "2 0 0.06\n",
      "2 1 0.18\n",
      "2 2 0.47\n",
      "2 3 0.29\n",
      "---\n",
      "3 0 0.00\n",
      "3 1 0.03\n",
      "3 2 0.01\n",
      "3 3 0.96\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# print(Cm[0])\n",
    "nd = [None]*4\n",
    "for i,nd in enumerate (Cm):\n",
    "    for j in range (len(nd)):\n",
    "        \n",
    "        print(i,j,\"%0.2f\"%(nd[j]/np.sum(nd)))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
