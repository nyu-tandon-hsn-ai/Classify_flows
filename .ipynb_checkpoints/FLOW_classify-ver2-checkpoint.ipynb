{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import keras\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = np.asarray(df[name], dtype = np.float).mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = np.asarray(df[name], dtype = np.float).std()\n",
    "\n",
    "    df[name] = (np.asarray(df[name], dtype = np.float) - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read df_ 12400 rows.\n",
      "Read df_test 5315 rows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "\n",
    "df_ = pd.read_csv(\"./train_70%.csv\")\n",
    "df_test = pd.read_csv(\"./test_30%.csv\")\n",
    "print(\"Read df_ {} rows.\".format(len(df_)))\n",
    "print(\"Read df_test {} rows.\".format(len(df_test)))\n",
    "#print(\"Read {} rows.\".format(len(df1)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df_ = df_.drop(df_.columns[0], axis=1)\n",
    "df_test = df_test.drop(df_test.columns[0], axis=1)\n",
    "\n",
    "df_.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "df_test.dropna(inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg(pkt_len)</th>\n",
       "      <th>stddev(pkt_len)</th>\n",
       "      <th>fb_ratio</th>\n",
       "      <th>inter_arrival_time</th>\n",
       "      <th>pkt_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>is_tcp</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>476.744186</td>\n",
       "      <td>455.723658</td>\n",
       "      <td>1.866667</td>\n",
       "      <td>0.023741</td>\n",
       "      <td>43</td>\n",
       "      <td>0.997141</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176.956044</td>\n",
       "      <td>330.176448</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>91</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>582.329457</td>\n",
       "      <td>374.212257</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>258</td>\n",
       "      <td>0.990259</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.193262</td>\n",
       "      <td>2</td>\n",
       "      <td>0.193262</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>472.983051</td>\n",
       "      <td>566.781150</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>118</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>652.591078</td>\n",
       "      <td>402.699413</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>269</td>\n",
       "      <td>0.974936</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>813.751429</td>\n",
       "      <td>343.954990</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>350</td>\n",
       "      <td>0.992912</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409896</td>\n",
       "      <td>0</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.100302</td>\n",
       "      <td>3</td>\n",
       "      <td>0.200603</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.875000</td>\n",
       "      <td>10.960155</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>8</td>\n",
       "      <td>0.008473</td>\n",
       "      <td>1</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>148.538462</td>\n",
       "      <td>398.434983</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.075125</td>\n",
       "      <td>13</td>\n",
       "      <td>0.901495</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1418.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.198235</td>\n",
       "      <td>1</td>\n",
       "      <td>download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>593.207407</td>\n",
       "      <td>591.651522</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>135</td>\n",
       "      <td>0.987934</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>807.232432</td>\n",
       "      <td>325.236066</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>370</td>\n",
       "      <td>0.999556</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>367.526316</td>\n",
       "      <td>596.137323</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>19</td>\n",
       "      <td>0.272225</td>\n",
       "      <td>1</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>358.783019</td>\n",
       "      <td>491.921829</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.009111</td>\n",
       "      <td>106</td>\n",
       "      <td>0.956677</td>\n",
       "      <td>0</td>\n",
       "      <td>voip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.735776</td>\n",
       "      <td>1</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>93</td>\n",
       "      <td>0.992447</td>\n",
       "      <td>1</td>\n",
       "      <td>streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100.444444</td>\n",
       "      <td>89.286213</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.118348</td>\n",
       "      <td>9</td>\n",
       "      <td>0.946786</td>\n",
       "      <td>0</td>\n",
       "      <td>game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1262.458244</td>\n",
       "      <td>130.037152</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>467</td>\n",
       "      <td>0.984296</td>\n",
       "      <td>1</td>\n",
       "      <td>streaming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    avg(pkt_len)  stddev(pkt_len)  fb_ratio  inter_arrival_time  pkt_count  \\\n",
       "0     476.744186       455.723658  1.866667            0.023741         43   \n",
       "1     176.956044       330.176448 -1.000000            0.011097         91   \n",
       "2     582.329457       374.212257 -1.000000            0.003853        258   \n",
       "3       0.000000         0.000000 -1.000000            0.193262          2   \n",
       "4     472.983051       566.781150 -1.000000            0.008524        118   \n",
       "5     652.591078       402.699413 -1.000000            0.003638        269   \n",
       "6     813.751429       343.954990 -1.000000            0.002845        350   \n",
       "7      30.000000        -1.000000 -1.000000           -1.000000          1   \n",
       "8       0.000000         0.000000 -1.000000            0.100302          3   \n",
       "9       3.875000        10.960155  1.000000            0.001210          8   \n",
       "10    148.538462       398.434983  1.166667            0.075125         13   \n",
       "11   1418.000000         0.000000 -1.000000            0.000198       1000   \n",
       "12    593.207407       591.651522 -1.000000            0.007373        135   \n",
       "13    807.232432       325.236066 -1.000000            0.002709        370   \n",
       "14    367.526316       596.137323  0.900000            0.015124         19   \n",
       "15    358.783019       491.921829 -1.000000            0.009111        106   \n",
       "16     90.000000        -1.000000 -1.000000           -1.000000          1   \n",
       "17      0.000000         0.000000 -1.000000            0.010787         93   \n",
       "18    100.444444        89.286213 -1.000000            0.118348          9   \n",
       "19   1262.458244       130.037152 -1.000000            0.002112        467   \n",
       "\n",
       "    duration  is_tcp    outcome  \n",
       "0   0.997141       0       voip  \n",
       "1   0.998755       0       voip  \n",
       "2   0.990259       0       voip  \n",
       "3   0.193262       1       voip  \n",
       "4   0.997261       0       voip  \n",
       "5   0.974936       0       voip  \n",
       "6   0.992912       0       voip  \n",
       "7   0.409896       0       game  \n",
       "8   0.200603       1       voip  \n",
       "9   0.008473       1       game  \n",
       "10  0.901495       1       voip  \n",
       "11  0.198235       1   download  \n",
       "12  0.987934       0       voip  \n",
       "13  0.999556       0       voip  \n",
       "14  0.272225       1       voip  \n",
       "15  0.956677       0       voip  \n",
       "16  0.735776       1       game  \n",
       "17  0.992447       1  streaming  \n",
       "18  0.946786       0       game  \n",
       "19  0.984296       1  streaming  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['download', 'game', 'streaming', 'voip'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_numeric_zscore(df_, 'fb_ratio')\n",
    "encode_numeric_zscore(df_, 'pkt_count')\n",
    "encode_numeric_zscore(df_, 'inter_arrival_time')\n",
    "encode_numeric_zscore(df_, 'stddev(pkt_len)')\n",
    "encode_text_index(df_, 'outcome')\n",
    "\n",
    "encode_numeric_zscore(df_test, 'fb_ratio')\n",
    "encode_numeric_zscore(df_test, 'pkt_count')\n",
    "encode_numeric_zscore(df_test, 'inter_arrival_time')\n",
    "encode_numeric_zscore(df_test, 'stddev(pkt_len)')\n",
    "encode_text_index(df_test, 'outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = to_xy(df_, 'outcome')\n",
    "x_test, y_test = to_xy(df_test, 'outcome')\n",
    "# x, y = to_xy(df_, ' Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg(pkt_len)</th>\n",
       "      <th>stddev(pkt_len)</th>\n",
       "      <th>fb_ratio</th>\n",
       "      <th>inter_arrival_time</th>\n",
       "      <th>pkt_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>is_tcp</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>476.744186</td>\n",
       "      <td>1.129337</td>\n",
       "      <td>2.921746</td>\n",
       "      <td>0.218888</td>\n",
       "      <td>-0.233498</td>\n",
       "      <td>0.997141</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176.956044</td>\n",
       "      <td>0.542150</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.179623</td>\n",
       "      <td>0.037789</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>582.329457</td>\n",
       "      <td>0.748107</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.157127</td>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.990259</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.002091</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.745324</td>\n",
       "      <td>-0.465222</td>\n",
       "      <td>0.193262</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>472.983051</td>\n",
       "      <td>1.648754</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.171630</td>\n",
       "      <td>0.190388</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>652.591078</td>\n",
       "      <td>0.881341</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.156458</td>\n",
       "      <td>1.043812</td>\n",
       "      <td>0.974936</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>813.751429</td>\n",
       "      <td>0.606593</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.153996</td>\n",
       "      <td>1.501609</td>\n",
       "      <td>0.992912</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>-1.006768</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>-2.960277</td>\n",
       "      <td>-0.470874</td>\n",
       "      <td>0.409896</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.002091</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.456641</td>\n",
       "      <td>-0.459570</td>\n",
       "      <td>0.200603</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.875000</td>\n",
       "      <td>-0.950830</td>\n",
       "      <td>1.867447</td>\n",
       "      <td>0.148920</td>\n",
       "      <td>-0.431311</td>\n",
       "      <td>0.008473</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>148.538462</td>\n",
       "      <td>0.861397</td>\n",
       "      <td>2.070197</td>\n",
       "      <td>0.378456</td>\n",
       "      <td>-0.403052</td>\n",
       "      <td>0.901495</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1418.000000</td>\n",
       "      <td>-1.002091</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.145777</td>\n",
       "      <td>5.175288</td>\n",
       "      <td>0.198235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>593.207407</td>\n",
       "      <td>1.765074</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.168056</td>\n",
       "      <td>0.286469</td>\n",
       "      <td>0.987934</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>807.232432</td>\n",
       "      <td>0.519044</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.153573</td>\n",
       "      <td>1.614645</td>\n",
       "      <td>0.999556</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>367.526316</td>\n",
       "      <td>1.786054</td>\n",
       "      <td>1.745797</td>\n",
       "      <td>0.192126</td>\n",
       "      <td>-0.369141</td>\n",
       "      <td>0.272225</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>358.783019</td>\n",
       "      <td>1.298636</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.173455</td>\n",
       "      <td>0.122566</td>\n",
       "      <td>0.956677</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>-1.006768</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>-2.960277</td>\n",
       "      <td>-0.470874</td>\n",
       "      <td>0.735776</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.002091</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.178661</td>\n",
       "      <td>0.049093</td>\n",
       "      <td>0.992447</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100.444444</td>\n",
       "      <td>-0.584497</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.512684</td>\n",
       "      <td>-0.425660</td>\n",
       "      <td>0.946786</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1262.458244</td>\n",
       "      <td>-0.393905</td>\n",
       "      <td>-0.565552</td>\n",
       "      <td>0.151720</td>\n",
       "      <td>2.162871</td>\n",
       "      <td>0.984296</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    avg(pkt_len)  stddev(pkt_len)  fb_ratio  inter_arrival_time  pkt_count  \\\n",
       "0     476.744186         1.129337  2.921746            0.218888  -0.233498   \n",
       "1     176.956044         0.542150 -0.565552            0.179623   0.037789   \n",
       "2     582.329457         0.748107 -0.565552            0.157127   0.981642   \n",
       "3       0.000000        -1.002091 -0.565552            0.745324  -0.465222   \n",
       "4     472.983051         1.648754 -0.565552            0.171630   0.190388   \n",
       "5     652.591078         0.881341 -0.565552            0.156458   1.043812   \n",
       "6     813.751429         0.606593 -0.565552            0.153996   1.501609   \n",
       "7      30.000000        -1.006768 -0.565552           -2.960277  -0.470874   \n",
       "8       0.000000        -1.002091 -0.565552            0.456641  -0.459570   \n",
       "9       3.875000        -0.950830  1.867447            0.148920  -0.431311   \n",
       "10    148.538462         0.861397  2.070197            0.378456  -0.403052   \n",
       "11   1418.000000        -1.002091 -0.565552            0.145777   5.175288   \n",
       "12    593.207407         1.765074 -0.565552            0.168056   0.286469   \n",
       "13    807.232432         0.519044 -0.565552            0.153573   1.614645   \n",
       "14    367.526316         1.786054  1.745797            0.192126  -0.369141   \n",
       "15    358.783019         1.298636 -0.565552            0.173455   0.122566   \n",
       "16     90.000000        -1.006768 -0.565552           -2.960277  -0.470874   \n",
       "17      0.000000        -1.002091 -0.565552            0.178661   0.049093   \n",
       "18    100.444444        -0.584497 -0.565552            0.512684  -0.425660   \n",
       "19   1262.458244        -0.393905 -0.565552            0.151720   2.162871   \n",
       "\n",
       "    duration  is_tcp  outcome  \n",
       "0   0.997141       0        3  \n",
       "1   0.998755       0        3  \n",
       "2   0.990259       0        3  \n",
       "3   0.193262       1        3  \n",
       "4   0.997261       0        3  \n",
       "5   0.974936       0        3  \n",
       "6   0.992912       0        3  \n",
       "7   0.409896       0        1  \n",
       "8   0.200603       1        3  \n",
       "9   0.008473       1        1  \n",
       "10  0.901495       1        3  \n",
       "11  0.198235       1        0  \n",
       "12  0.987934       0        3  \n",
       "13  0.999556       0        3  \n",
       "14  0.272225       1        3  \n",
       "15  0.956677       0        3  \n",
       "16  0.735776       1        1  \n",
       "17  0.992447       1        2  \n",
       "18  0.946786       0        1  \n",
       "19  0.984296       1        2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(y_train.shape[1],activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 196\n",
      "Trainable params: 196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # TODO:  Create two empty lists, self.loss and self.val_acc\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.val_acc = []\n",
    "        self.val_loss = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # TODO:  This is called at the end of each batch.  \n",
    "        # Add the loss in logs.get('loss') to the loss list\n",
    "        loss = logs.get('loss')\n",
    "        acc = logs.get('acc')\n",
    "        self.losses.append(loss)\n",
    "        self.accs.append(acc)\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # TODO:  This is called at the end of each epoch.  \n",
    "        # Add the test accuracy in logs.get('loss') to the val_acc list\n",
    "        val_acc = logs.get('val_acc')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        self.val_acc.append(val_acc)\n",
    "        self.val_loss.append(val_loss)\n",
    "\n",
    "# Create an instance of the history callback\n",
    "history_cb = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12400 samples, validate on 5315 samples\n",
      "Epoch 1/300\n",
      "12400/12400 [==============================] - 1s - loss: 1.3895 - acc: 0.2904 - val_loss: 1.0754 - val_acc: 0.4024\n",
      "Epoch 2/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.9063 - acc: 0.4083 - val_loss: 0.8266 - val_acc: 0.4303\n",
      "Epoch 3/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.7553 - acc: 0.6004 - val_loss: 0.6951 - val_acc: 0.7398\n",
      "Epoch 4/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.5889 - acc: 0.7931 - val_loss: 0.5665 - val_acc: 0.7897\n",
      "Epoch 5/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.5647 - acc: 0.7903 - val_loss: 0.5582 - val_acc: 0.7947\n",
      "Epoch 6/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.5421 - acc: 0.7965 - val_loss: 0.5350 - val_acc: 0.7991\n",
      "Epoch 7/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.5346 - acc: 0.7998 - val_loss: 0.5242 - val_acc: 0.7992\n",
      "Epoch 8/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.5223 - acc: 0.8046 - val_loss: 0.5430 - val_acc: 0.8013\n",
      "Epoch 9/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.5143 - acc: 0.8102 - val_loss: 0.5264 - val_acc: 0.7987\n",
      "Epoch 10/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.5099 - acc: 0.8116 - val_loss: 0.5054 - val_acc: 0.8149\n",
      "Epoch 11/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.5009 - acc: 0.8186 - val_loss: 0.4975 - val_acc: 0.8175\n",
      "Epoch 12/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4930 - acc: 0.8254 - val_loss: 0.5027 - val_acc: 0.8122\n",
      "Epoch 13/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4879 - acc: 0.8260 - val_loss: 0.4942 - val_acc: 0.8260\n",
      "Epoch 14/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4785 - acc: 0.8278 - val_loss: 0.4775 - val_acc: 0.8267\n",
      "Epoch 15/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4730 - acc: 0.8277 - val_loss: 0.4953 - val_acc: 0.8230\n",
      "Epoch 16/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4642 - acc: 0.8314 - val_loss: 0.4683 - val_acc: 0.8262\n",
      "Epoch 17/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4583 - acc: 0.8314 - val_loss: 0.4748 - val_acc: 0.8273\n",
      "Epoch 18/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4557 - acc: 0.8325 - val_loss: 0.4575 - val_acc: 0.8292\n",
      "Epoch 19/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4467 - acc: 0.8326 - val_loss: 0.4709 - val_acc: 0.8177\n",
      "Epoch 20/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4389 - acc: 0.8341 - val_loss: 0.4628 - val_acc: 0.8305\n",
      "Epoch 21/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4354 - acc: 0.8331 - val_loss: 0.4408 - val_acc: 0.8269\n",
      "Epoch 22/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4291 - acc: 0.8341 - val_loss: 0.4542 - val_acc: 0.8275\n",
      "Epoch 23/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4263 - acc: 0.8331 - val_loss: 0.4263 - val_acc: 0.8314\n",
      "Epoch 24/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4170 - acc: 0.8334 - val_loss: 0.4414 - val_acc: 0.8235\n",
      "Epoch 25/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4185 - acc: 0.8327 - val_loss: 0.4330 - val_acc: 0.8277\n",
      "Epoch 26/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4195 - acc: 0.8328 - val_loss: 0.4274 - val_acc: 0.8324\n",
      "Epoch 27/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4131 - acc: 0.8327 - val_loss: 0.4362 - val_acc: 0.8324\n",
      "Epoch 28/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4077 - acc: 0.8340 - val_loss: 0.4135 - val_acc: 0.8303\n",
      "Epoch 29/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4089 - acc: 0.8342 - val_loss: 0.4223 - val_acc: 0.8286\n",
      "Epoch 30/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4041 - acc: 0.8340 - val_loss: 0.4207 - val_acc: 0.8318\n",
      "Epoch 31/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4041 - acc: 0.8339 - val_loss: 0.4176 - val_acc: 0.8301\n",
      "Epoch 32/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4021 - acc: 0.8331 - val_loss: 0.4064 - val_acc: 0.8371\n",
      "Epoch 33/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4056 - acc: 0.8356 - val_loss: 0.4096 - val_acc: 0.8307\n",
      "Epoch 34/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4034 - acc: 0.8323 - val_loss: 0.4132 - val_acc: 0.8314\n",
      "Epoch 35/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.4010 - acc: 0.8337 - val_loss: 0.4102 - val_acc: 0.8295\n",
      "Epoch 36/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3982 - acc: 0.8362 - val_loss: 0.4075 - val_acc: 0.8303\n",
      "Epoch 37/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3983 - acc: 0.8377 - val_loss: 0.4030 - val_acc: 0.8275\n",
      "Epoch 38/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3963 - acc: 0.8375 - val_loss: 0.4082 - val_acc: 0.8331\n",
      "Epoch 39/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3960 - acc: 0.8352 - val_loss: 0.4258 - val_acc: 0.8303\n",
      "Epoch 40/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3943 - acc: 0.8347 - val_loss: 0.4202 - val_acc: 0.8331\n",
      "Epoch 41/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3936 - acc: 0.8366 - val_loss: 0.4080 - val_acc: 0.8290\n",
      "Epoch 42/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3922 - acc: 0.8387 - val_loss: 0.4083 - val_acc: 0.8280\n",
      "Epoch 43/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3927 - acc: 0.8380 - val_loss: 0.4032 - val_acc: 0.8299\n",
      "Epoch 44/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3896 - acc: 0.8382 - val_loss: 0.3983 - val_acc: 0.8361\n",
      "Epoch 45/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3907 - acc: 0.8391 - val_loss: 0.4260 - val_acc: 0.8282\n",
      "Epoch 46/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3922 - acc: 0.8403 - val_loss: 0.3989 - val_acc: 0.8327\n",
      "Epoch 47/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3879 - acc: 0.8380 - val_loss: 0.4053 - val_acc: 0.8325\n",
      "Epoch 48/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3883 - acc: 0.8387 - val_loss: 0.4071 - val_acc: 0.8361\n",
      "Epoch 49/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3885 - acc: 0.8409 - val_loss: 0.4396 - val_acc: 0.8199\n",
      "Epoch 50/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3909 - acc: 0.8377 - val_loss: 0.4057 - val_acc: 0.8359\n",
      "Epoch 51/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3922 - acc: 0.8385 - val_loss: 0.4028 - val_acc: 0.8305\n",
      "Epoch 52/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3857 - acc: 0.8385 - val_loss: 0.4046 - val_acc: 0.8316\n",
      "Epoch 53/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3880 - acc: 0.8381 - val_loss: 0.3922 - val_acc: 0.8367\n",
      "Epoch 54/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3852 - acc: 0.8398 - val_loss: 0.4168 - val_acc: 0.8301\n",
      "Epoch 55/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3857 - acc: 0.8387 - val_loss: 0.4195 - val_acc: 0.8292\n",
      "Epoch 56/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3880 - acc: 0.8388 - val_loss: 0.4000 - val_acc: 0.8318\n",
      "Epoch 57/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3901 - acc: 0.8377 - val_loss: 0.4065 - val_acc: 0.8337\n",
      "Epoch 58/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3866 - acc: 0.8400 - val_loss: 0.4055 - val_acc: 0.8284\n",
      "Epoch 59/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3848 - acc: 0.8380 - val_loss: 0.4451 - val_acc: 0.8273\n",
      "Epoch 60/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3890 - acc: 0.8398 - val_loss: 0.3885 - val_acc: 0.8352\n",
      "Epoch 61/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3826 - acc: 0.8416 - val_loss: 0.4224 - val_acc: 0.8292\n",
      "Epoch 62/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3886 - acc: 0.8397 - val_loss: 0.3933 - val_acc: 0.8361\n",
      "Epoch 63/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3857 - acc: 0.8385 - val_loss: 0.3973 - val_acc: 0.8316\n",
      "Epoch 64/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400/12400 [==============================] - 1s - loss: 0.3840 - acc: 0.8396 - val_loss: 0.4223 - val_acc: 0.8299\n",
      "Epoch 65/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3847 - acc: 0.8397 - val_loss: 0.3995 - val_acc: 0.8352\n",
      "Epoch 66/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3874 - acc: 0.8391 - val_loss: 0.3967 - val_acc: 0.8356\n",
      "Epoch 67/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3845 - acc: 0.8402 - val_loss: 0.3909 - val_acc: 0.8331\n",
      "Epoch 68/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3836 - acc: 0.8387 - val_loss: 0.4030 - val_acc: 0.8290\n",
      "Epoch 69/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3838 - acc: 0.8384 - val_loss: 0.3888 - val_acc: 0.8331\n",
      "Epoch 70/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3826 - acc: 0.8399 - val_loss: 0.4056 - val_acc: 0.8312\n",
      "Epoch 71/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3840 - acc: 0.8399 - val_loss: 0.3863 - val_acc: 0.8342\n",
      "Epoch 72/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3834 - acc: 0.8402 - val_loss: 0.4111 - val_acc: 0.8348\n",
      "Epoch 73/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3815 - acc: 0.8399 - val_loss: 0.4021 - val_acc: 0.8273\n",
      "Epoch 74/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3822 - acc: 0.8372 - val_loss: 0.3852 - val_acc: 0.8386\n",
      "Epoch 75/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3824 - acc: 0.8381 - val_loss: 0.3995 - val_acc: 0.8376\n",
      "Epoch 76/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3836 - acc: 0.8399 - val_loss: 0.3900 - val_acc: 0.8346\n",
      "Epoch 77/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3812 - acc: 0.8398 - val_loss: 0.4005 - val_acc: 0.8301\n",
      "Epoch 78/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3789 - acc: 0.8404 - val_loss: 0.3894 - val_acc: 0.8342\n",
      "Epoch 79/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3824 - acc: 0.8380 - val_loss: 0.3928 - val_acc: 0.8374\n",
      "Epoch 80/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3814 - acc: 0.8390 - val_loss: 0.3994 - val_acc: 0.8356\n",
      "Epoch 81/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3790 - acc: 0.8402 - val_loss: 0.3924 - val_acc: 0.8339\n",
      "Epoch 82/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3803 - acc: 0.8385 - val_loss: 0.3908 - val_acc: 0.8376\n",
      "Epoch 83/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3844 - acc: 0.8387 - val_loss: 0.4043 - val_acc: 0.8299\n",
      "Epoch 84/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3790 - acc: 0.8404 - val_loss: 0.3851 - val_acc: 0.8324\n",
      "Epoch 85/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3806 - acc: 0.8403 - val_loss: 0.4036 - val_acc: 0.8327\n",
      "Epoch 86/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3803 - acc: 0.8377 - val_loss: 0.3997 - val_acc: 0.8382\n",
      "Epoch 87/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3788 - acc: 0.8384 - val_loss: 0.4035 - val_acc: 0.8350\n",
      "Epoch 88/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3809 - acc: 0.8379 - val_loss: 0.3940 - val_acc: 0.8361\n",
      "Epoch 89/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3806 - acc: 0.8388 - val_loss: 0.4012 - val_acc: 0.8363\n",
      "Epoch 90/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3788 - acc: 0.8394 - val_loss: 0.3902 - val_acc: 0.8333\n",
      "Epoch 91/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3796 - acc: 0.8409 - val_loss: 0.3923 - val_acc: 0.8290\n",
      "Epoch 92/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3786 - acc: 0.8393 - val_loss: 0.3863 - val_acc: 0.8346\n",
      "Epoch 93/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3806 - acc: 0.8390 - val_loss: 0.3882 - val_acc: 0.8354\n",
      "Epoch 94/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3780 - acc: 0.8422 - val_loss: 0.3874 - val_acc: 0.8365\n",
      "Epoch 95/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3781 - acc: 0.8406 - val_loss: 0.3989 - val_acc: 0.8367\n",
      "Epoch 96/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3781 - acc: 0.8395 - val_loss: 0.3904 - val_acc: 0.8384\n",
      "Epoch 97/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3818 - acc: 0.8405 - val_loss: 0.3864 - val_acc: 0.8325\n",
      "Epoch 98/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3773 - acc: 0.8390 - val_loss: 0.3974 - val_acc: 0.8309\n",
      "Epoch 99/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3773 - acc: 0.8402 - val_loss: 0.3992 - val_acc: 0.8314\n",
      "Epoch 100/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3777 - acc: 0.8420 - val_loss: 0.4013 - val_acc: 0.8309\n",
      "Epoch 101/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3823 - acc: 0.8395 - val_loss: 0.3885 - val_acc: 0.8374\n",
      "Epoch 102/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3780 - acc: 0.8406 - val_loss: 0.3887 - val_acc: 0.8344\n",
      "Epoch 103/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3780 - acc: 0.8395 - val_loss: 0.3864 - val_acc: 0.8301\n",
      "Epoch 104/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3795 - acc: 0.8406 - val_loss: 0.3849 - val_acc: 0.8365\n",
      "Epoch 105/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3758 - acc: 0.8402 - val_loss: 0.3843 - val_acc: 0.8356\n",
      "Epoch 106/300\n",
      "12400/12400 [==============================] - 0s - loss: 0.3777 - acc: 0.8397 - val_loss: 0.3877 - val_acc: 0.8380\n",
      "Epoch 107/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3782 - acc: 0.8398 - val_loss: 0.3865 - val_acc: 0.8361\n",
      "Epoch 108/300\n",
      "12400/12400 [==============================] - 0s - loss: 0.3779 - acc: 0.8398 - val_loss: 0.4120 - val_acc: 0.8305\n",
      "Epoch 109/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3791 - acc: 0.8410 - val_loss: 0.3862 - val_acc: 0.8348\n",
      "Epoch 110/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3767 - acc: 0.8402 - val_loss: 0.3944 - val_acc: 0.8254\n",
      "Epoch 111/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3758 - acc: 0.8389 - val_loss: 0.3966 - val_acc: 0.8292\n",
      "Epoch 112/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3766 - acc: 0.8390 - val_loss: 0.4022 - val_acc: 0.8295\n",
      "Epoch 113/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3792 - acc: 0.8392 - val_loss: 0.4012 - val_acc: 0.8314\n",
      "Epoch 114/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3764 - acc: 0.8399 - val_loss: 0.3872 - val_acc: 0.8337\n",
      "Epoch 115/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3762 - acc: 0.8390 - val_loss: 0.3918 - val_acc: 0.8277\n",
      "Epoch 116/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3772 - acc: 0.8399 - val_loss: 0.3849 - val_acc: 0.8305\n",
      "Epoch 117/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3788 - acc: 0.8387 - val_loss: 0.3820 - val_acc: 0.8329\n",
      "Epoch 118/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3809 - acc: 0.8402 - val_loss: 0.3819 - val_acc: 0.8314\n",
      "Epoch 119/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3770 - acc: 0.8398 - val_loss: 0.3769 - val_acc: 0.8354\n",
      "Epoch 120/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3761 - acc: 0.8398 - val_loss: 0.3882 - val_acc: 0.8350\n",
      "Epoch 121/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3756 - acc: 0.8413 - val_loss: 0.3809 - val_acc: 0.8380\n",
      "Epoch 122/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3787 - acc: 0.8400 - val_loss: 0.3905 - val_acc: 0.8309\n",
      "Epoch 123/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3786 - acc: 0.8390 - val_loss: 0.3854 - val_acc: 0.8337\n",
      "Epoch 124/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3742 - acc: 0.8411 - val_loss: 0.3865 - val_acc: 0.8374\n",
      "Epoch 125/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3795 - acc: 0.8394 - val_loss: 0.3849 - val_acc: 0.8354\n",
      "Epoch 126/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3748 - acc: 0.8409 - val_loss: 0.3791 - val_acc: 0.8333\n",
      "Epoch 127/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400/12400 [==============================] - 1s - loss: 0.3747 - acc: 0.8430 - val_loss: 0.3921 - val_acc: 0.8316\n",
      "Epoch 128/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3763 - acc: 0.8404 - val_loss: 0.3819 - val_acc: 0.8352\n",
      "Epoch 129/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3744 - acc: 0.8400 - val_loss: 0.3907 - val_acc: 0.8346\n",
      "Epoch 130/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3744 - acc: 0.8408 - val_loss: 0.3815 - val_acc: 0.8337\n",
      "Epoch 131/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3766 - acc: 0.8390 - val_loss: 0.3959 - val_acc: 0.8322\n",
      "Epoch 132/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3743 - acc: 0.8390 - val_loss: 0.3849 - val_acc: 0.8320\n",
      "Epoch 133/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3722 - acc: 0.8406 - val_loss: 0.3958 - val_acc: 0.8318\n",
      "Epoch 134/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3759 - acc: 0.8395 - val_loss: 0.3880 - val_acc: 0.8342\n",
      "Epoch 135/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3725 - acc: 0.8396 - val_loss: 0.3859 - val_acc: 0.8348\n",
      "Epoch 136/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3739 - acc: 0.8390 - val_loss: 0.3832 - val_acc: 0.8335\n",
      "Epoch 137/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3761 - acc: 0.8393 - val_loss: 0.3909 - val_acc: 0.8329\n",
      "Epoch 138/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3750 - acc: 0.8403 - val_loss: 0.3848 - val_acc: 0.8356\n",
      "Epoch 139/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3749 - acc: 0.8388 - val_loss: 0.3811 - val_acc: 0.8367\n",
      "Epoch 140/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3722 - acc: 0.8415 - val_loss: 0.4023 - val_acc: 0.8327\n",
      "Epoch 141/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3755 - acc: 0.8397 - val_loss: 0.3838 - val_acc: 0.8361\n",
      "Epoch 142/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3766 - acc: 0.8410 - val_loss: 0.3805 - val_acc: 0.8333\n",
      "Epoch 143/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3765 - acc: 0.8387 - val_loss: 0.3780 - val_acc: 0.8354\n",
      "Epoch 144/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3746 - acc: 0.8394 - val_loss: 0.3853 - val_acc: 0.8331\n",
      "Epoch 145/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3728 - acc: 0.8406 - val_loss: 0.3838 - val_acc: 0.8320\n",
      "Epoch 146/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3752 - acc: 0.8390 - val_loss: 0.3853 - val_acc: 0.8335\n",
      "Epoch 147/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3724 - acc: 0.8410 - val_loss: 0.3940 - val_acc: 0.8367\n",
      "Epoch 148/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3727 - acc: 0.8403 - val_loss: 0.3973 - val_acc: 0.8325\n",
      "Epoch 149/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3741 - acc: 0.8406 - val_loss: 0.3772 - val_acc: 0.8367\n",
      "Epoch 150/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3732 - acc: 0.8402 - val_loss: 0.3842 - val_acc: 0.8359\n",
      "Epoch 151/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3748 - acc: 0.8415 - val_loss: 0.3792 - val_acc: 0.8350\n",
      "Epoch 152/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3730 - acc: 0.8410 - val_loss: 0.3774 - val_acc: 0.8356\n",
      "Epoch 153/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3718 - acc: 0.8390 - val_loss: 0.3850 - val_acc: 0.8363\n",
      "Epoch 154/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3722 - acc: 0.8417 - val_loss: 0.3849 - val_acc: 0.8337\n",
      "Epoch 155/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3728 - acc: 0.8413 - val_loss: 0.3794 - val_acc: 0.8374\n",
      "Epoch 156/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3717 - acc: 0.8406 - val_loss: 0.3801 - val_acc: 0.8341\n",
      "Epoch 157/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3724 - acc: 0.8385 - val_loss: 0.3790 - val_acc: 0.8346\n",
      "Epoch 158/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3727 - acc: 0.8409 - val_loss: 0.3838 - val_acc: 0.8325\n",
      "Epoch 159/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3707 - acc: 0.8402 - val_loss: 0.3804 - val_acc: 0.8350\n",
      "Epoch 160/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3726 - acc: 0.8415 - val_loss: 0.3895 - val_acc: 0.8297\n",
      "Epoch 161/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3720 - acc: 0.8405 - val_loss: 0.3806 - val_acc: 0.8357\n",
      "Epoch 162/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3740 - acc: 0.8419 - val_loss: 0.3852 - val_acc: 0.8333\n",
      "Epoch 163/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3712 - acc: 0.8420 - val_loss: 0.3780 - val_acc: 0.8367\n",
      "Epoch 164/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3739 - acc: 0.8407 - val_loss: 0.3813 - val_acc: 0.8380\n",
      "Epoch 165/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3706 - acc: 0.8411 - val_loss: 0.3772 - val_acc: 0.8357\n",
      "Epoch 166/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3714 - acc: 0.8414 - val_loss: 0.3877 - val_acc: 0.8322\n",
      "Epoch 167/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3729 - acc: 0.8406 - val_loss: 0.3801 - val_acc: 0.8369\n",
      "Epoch 168/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3731 - acc: 0.8399 - val_loss: 0.3801 - val_acc: 0.8346\n",
      "Epoch 169/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3701 - acc: 0.8413 - val_loss: 0.3798 - val_acc: 0.8310\n",
      "Epoch 170/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3706 - acc: 0.8417 - val_loss: 0.3753 - val_acc: 0.8369\n",
      "Epoch 171/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3701 - acc: 0.8394 - val_loss: 0.3831 - val_acc: 0.8365\n",
      "Epoch 172/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3704 - acc: 0.8440 - val_loss: 0.3713 - val_acc: 0.8374\n",
      "Epoch 173/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3723 - acc: 0.8411 - val_loss: 0.3764 - val_acc: 0.8386\n",
      "Epoch 174/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3706 - acc: 0.8423 - val_loss: 0.3761 - val_acc: 0.8341\n",
      "Epoch 175/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3724 - acc: 0.8419 - val_loss: 0.3734 - val_acc: 0.8386\n",
      "Epoch 176/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3712 - acc: 0.8429 - val_loss: 0.3744 - val_acc: 0.8376\n",
      "Epoch 177/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3725 - acc: 0.8410 - val_loss: 0.3839 - val_acc: 0.8344\n",
      "Epoch 178/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3685 - acc: 0.8434 - val_loss: 0.3793 - val_acc: 0.8354\n",
      "Epoch 179/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3689 - acc: 0.8413 - val_loss: 0.4079 - val_acc: 0.8307\n",
      "Epoch 180/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3720 - acc: 0.8432 - val_loss: 0.3950 - val_acc: 0.8421\n",
      "Epoch 181/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3689 - acc: 0.8444 - val_loss: 0.3745 - val_acc: 0.8453\n",
      "Epoch 182/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3706 - acc: 0.8436 - val_loss: 0.3984 - val_acc: 0.8294\n",
      "Epoch 183/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3688 - acc: 0.8404 - val_loss: 0.3909 - val_acc: 0.8382\n",
      "Epoch 184/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3723 - acc: 0.8413 - val_loss: 0.3791 - val_acc: 0.8395\n",
      "Epoch 185/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3692 - acc: 0.8430 - val_loss: 0.3999 - val_acc: 0.8429\n",
      "Epoch 186/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3693 - acc: 0.8440 - val_loss: 0.3837 - val_acc: 0.8357\n",
      "Epoch 187/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3707 - acc: 0.8431 - val_loss: 0.3779 - val_acc: 0.8354\n",
      "Epoch 188/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3684 - acc: 0.8432 - val_loss: 0.3781 - val_acc: 0.8369\n",
      "Epoch 189/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3680 - acc: 0.8434 - val_loss: 0.3888 - val_acc: 0.8386\n",
      "Epoch 190/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400/12400 [==============================] - 1s - loss: 0.3687 - acc: 0.8415 - val_loss: 0.3850 - val_acc: 0.8346\n",
      "Epoch 191/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3700 - acc: 0.8434 - val_loss: 0.3787 - val_acc: 0.8335\n",
      "Epoch 192/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3690 - acc: 0.8427 - val_loss: 0.3868 - val_acc: 0.8401\n",
      "Epoch 193/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3709 - acc: 0.8450 - val_loss: 0.3752 - val_acc: 0.8357\n",
      "Epoch 194/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3694 - acc: 0.8445 - val_loss: 0.3881 - val_acc: 0.8348\n",
      "Epoch 195/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3688 - acc: 0.8433 - val_loss: 0.3769 - val_acc: 0.8374\n",
      "Epoch 196/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3713 - acc: 0.8439 - val_loss: 0.3764 - val_acc: 0.8410\n",
      "Epoch 197/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3660 - acc: 0.8434 - val_loss: 0.3723 - val_acc: 0.8378\n",
      "Epoch 198/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3672 - acc: 0.8448 - val_loss: 0.3846 - val_acc: 0.8339\n",
      "Epoch 199/300\n",
      "12400/12400 [==============================] - 0s - loss: 0.3679 - acc: 0.8454 - val_loss: 0.3906 - val_acc: 0.8418\n",
      "Epoch 200/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3682 - acc: 0.8431 - val_loss: 0.3875 - val_acc: 0.8335\n",
      "Epoch 201/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3686 - acc: 0.8444 - val_loss: 0.3829 - val_acc: 0.8295\n",
      "Epoch 202/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3694 - acc: 0.8447 - val_loss: 0.3866 - val_acc: 0.8401\n",
      "Epoch 203/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3720 - acc: 0.8435 - val_loss: 0.3712 - val_acc: 0.8382\n",
      "Epoch 204/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3662 - acc: 0.8467 - val_loss: 0.3789 - val_acc: 0.8339\n",
      "Epoch 205/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3672 - acc: 0.8448 - val_loss: 0.3768 - val_acc: 0.8442\n",
      "Epoch 206/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3693 - acc: 0.8440 - val_loss: 0.3742 - val_acc: 0.8399\n",
      "Epoch 207/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3641 - acc: 0.8432 - val_loss: 0.3745 - val_acc: 0.8378\n",
      "Epoch 208/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3672 - acc: 0.8454 - val_loss: 0.3818 - val_acc: 0.8448\n",
      "Epoch 209/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3698 - acc: 0.8429 - val_loss: 0.3872 - val_acc: 0.8446\n",
      "Epoch 210/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3690 - acc: 0.8465 - val_loss: 0.3844 - val_acc: 0.8359\n",
      "Epoch 211/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3660 - acc: 0.8455 - val_loss: 0.3872 - val_acc: 0.8333\n",
      "Epoch 212/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3671 - acc: 0.8434 - val_loss: 0.3750 - val_acc: 0.8356\n",
      "Epoch 213/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3674 - acc: 0.8438 - val_loss: 0.3752 - val_acc: 0.8391\n",
      "Epoch 214/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3662 - acc: 0.8433 - val_loss: 0.3770 - val_acc: 0.8327\n",
      "Epoch 215/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3692 - acc: 0.8456 - val_loss: 0.3754 - val_acc: 0.8331\n",
      "Epoch 216/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3673 - acc: 0.8423 - val_loss: 0.3714 - val_acc: 0.8354\n",
      "Epoch 217/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3662 - acc: 0.8453 - val_loss: 0.3781 - val_acc: 0.8369\n",
      "Epoch 218/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3681 - acc: 0.8427 - val_loss: 0.3836 - val_acc: 0.8376\n",
      "Epoch 219/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3661 - acc: 0.8450 - val_loss: 0.3907 - val_acc: 0.8324\n",
      "Epoch 220/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3665 - acc: 0.8440 - val_loss: 0.3894 - val_acc: 0.8335\n",
      "Epoch 221/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3674 - acc: 0.8447 - val_loss: 0.3733 - val_acc: 0.8346\n",
      "Epoch 222/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3677 - acc: 0.8470 - val_loss: 0.3741 - val_acc: 0.8335\n",
      "Epoch 223/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3651 - acc: 0.8453 - val_loss: 0.3710 - val_acc: 0.8371\n",
      "Epoch 224/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3715 - acc: 0.8460 - val_loss: 0.3797 - val_acc: 0.8322\n",
      "Epoch 225/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3658 - acc: 0.8450 - val_loss: 0.3737 - val_acc: 0.8359\n",
      "Epoch 226/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3668 - acc: 0.8436 - val_loss: 0.3761 - val_acc: 0.8363\n",
      "Epoch 227/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3649 - acc: 0.8460 - val_loss: 0.3838 - val_acc: 0.8414\n",
      "Epoch 228/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3685 - acc: 0.8456 - val_loss: 0.3820 - val_acc: 0.8339\n",
      "Epoch 229/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3658 - acc: 0.8455 - val_loss: 0.3746 - val_acc: 0.8314\n",
      "Epoch 230/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3698 - acc: 0.8445 - val_loss: 0.3768 - val_acc: 0.8329\n",
      "Epoch 231/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3663 - acc: 0.8456 - val_loss: 0.3860 - val_acc: 0.8363\n",
      "Epoch 232/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3654 - acc: 0.8473 - val_loss: 0.3742 - val_acc: 0.8337\n",
      "Epoch 233/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3655 - acc: 0.8446 - val_loss: 0.3763 - val_acc: 0.8442\n",
      "Epoch 234/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3666 - acc: 0.8438 - val_loss: 0.3819 - val_acc: 0.8397\n",
      "Epoch 235/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3681 - acc: 0.8462 - val_loss: 0.3741 - val_acc: 0.8350\n",
      "Epoch 236/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3627 - acc: 0.8452 - val_loss: 0.3854 - val_acc: 0.8380\n",
      "Epoch 237/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3652 - acc: 0.8467 - val_loss: 0.3828 - val_acc: 0.8425\n",
      "Epoch 238/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3668 - acc: 0.8432 - val_loss: 0.3924 - val_acc: 0.8437\n",
      "Epoch 239/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3639 - acc: 0.8462 - val_loss: 0.3790 - val_acc: 0.8425\n",
      "Epoch 240/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3676 - acc: 0.8459 - val_loss: 0.3747 - val_acc: 0.8452\n",
      "Epoch 241/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3645 - acc: 0.8446 - val_loss: 0.3835 - val_acc: 0.8365\n",
      "Epoch 242/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3664 - acc: 0.8458 - val_loss: 0.3737 - val_acc: 0.8453\n",
      "Epoch 243/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3677 - acc: 0.8442 - val_loss: 0.3817 - val_acc: 0.8438\n",
      "Epoch 244/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3649 - acc: 0.8490 - val_loss: 0.3766 - val_acc: 0.8405\n",
      "Epoch 245/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3636 - acc: 0.8468 - val_loss: 0.3710 - val_acc: 0.8438\n",
      "Epoch 246/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3656 - acc: 0.8435 - val_loss: 0.3787 - val_acc: 0.8444\n",
      "Epoch 247/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3654 - acc: 0.8469 - val_loss: 0.3746 - val_acc: 0.8418\n",
      "Epoch 248/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3634 - acc: 0.8443 - val_loss: 0.3742 - val_acc: 0.8401\n",
      "Epoch 249/300\n",
      "12400/12400 [==============================] - 0s - loss: 0.3646 - acc: 0.8472 - val_loss: 0.3716 - val_acc: 0.8416\n",
      "Epoch 250/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3642 - acc: 0.8484 - val_loss: 0.3814 - val_acc: 0.8380\n",
      "Epoch 251/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3647 - acc: 0.8444 - val_loss: 0.3690 - val_acc: 0.8324\n",
      "Epoch 252/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3641 - acc: 0.8477 - val_loss: 0.3741 - val_acc: 0.8393\n",
      "Epoch 253/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400/12400 [==============================] - 0s - loss: 0.3631 - acc: 0.8445 - val_loss: 0.3702 - val_acc: 0.8463\n",
      "Epoch 254/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3644 - acc: 0.8445 - val_loss: 0.3805 - val_acc: 0.8399\n",
      "Epoch 255/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3650 - acc: 0.8432 - val_loss: 0.3722 - val_acc: 0.8463\n",
      "Epoch 256/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3647 - acc: 0.8476 - val_loss: 0.3688 - val_acc: 0.8389\n",
      "Epoch 257/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3620 - acc: 0.8448 - val_loss: 0.3713 - val_acc: 0.8389\n",
      "Epoch 258/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3650 - acc: 0.8464 - val_loss: 0.3784 - val_acc: 0.8448\n",
      "Epoch 259/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3636 - acc: 0.8455 - val_loss: 0.3783 - val_acc: 0.8427\n",
      "Epoch 260/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3626 - acc: 0.8452 - val_loss: 0.3932 - val_acc: 0.8395\n",
      "Epoch 261/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3658 - acc: 0.8448 - val_loss: 0.3784 - val_acc: 0.8403\n",
      "Epoch 262/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3628 - acc: 0.8464 - val_loss: 0.3740 - val_acc: 0.8367\n",
      "Epoch 263/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3641 - acc: 0.8491 - val_loss: 0.3741 - val_acc: 0.8386\n",
      "Epoch 264/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3657 - acc: 0.8456 - val_loss: 0.3749 - val_acc: 0.8325\n",
      "Epoch 265/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3641 - acc: 0.8436 - val_loss: 0.3715 - val_acc: 0.8371\n",
      "Epoch 266/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3662 - acc: 0.8457 - val_loss: 0.3790 - val_acc: 0.8329\n",
      "Epoch 267/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3614 - acc: 0.8473 - val_loss: 0.3820 - val_acc: 0.8468\n",
      "Epoch 268/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3648 - acc: 0.8452 - val_loss: 0.3686 - val_acc: 0.8344\n",
      "Epoch 269/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3628 - acc: 0.8441 - val_loss: 0.3832 - val_acc: 0.8457\n",
      "Epoch 270/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3653 - acc: 0.8478 - val_loss: 0.3741 - val_acc: 0.8465\n",
      "Epoch 271/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3658 - acc: 0.8438 - val_loss: 0.3875 - val_acc: 0.8421\n",
      "Epoch 272/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3649 - acc: 0.8441 - val_loss: 0.3799 - val_acc: 0.8438\n",
      "Epoch 273/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3606 - acc: 0.8476 - val_loss: 0.3776 - val_acc: 0.8410\n",
      "Epoch 274/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3599 - acc: 0.8469 - val_loss: 0.3681 - val_acc: 0.8452\n",
      "Epoch 275/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3633 - acc: 0.8479 - val_loss: 0.3833 - val_acc: 0.8337\n",
      "Epoch 276/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3645 - acc: 0.8459 - val_loss: 0.3814 - val_acc: 0.8455\n",
      "Epoch 277/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3641 - acc: 0.8472 - val_loss: 0.3843 - val_acc: 0.8442\n",
      "Epoch 278/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3632 - acc: 0.8489 - val_loss: 0.3722 - val_acc: 0.8359\n",
      "Epoch 279/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3610 - acc: 0.8448 - val_loss: 0.3863 - val_acc: 0.8327\n",
      "Epoch 280/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3635 - acc: 0.8463 - val_loss: 0.3718 - val_acc: 0.8389\n",
      "Epoch 281/300\n",
      "12400/12400 [==============================] - ETA: 0s - loss: 0.3639 - acc: 0.846 - 1s - loss: 0.3621 - acc: 0.8475 - val_loss: 0.3803 - val_acc: 0.8318\n",
      "Epoch 282/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3609 - acc: 0.8459 - val_loss: 0.3699 - val_acc: 0.8403\n",
      "Epoch 283/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3611 - acc: 0.8460 - val_loss: 0.3760 - val_acc: 0.8406\n",
      "Epoch 284/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3606 - acc: 0.8467 - val_loss: 0.3750 - val_acc: 0.8423\n",
      "Epoch 285/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3617 - acc: 0.8477 - val_loss: 0.3714 - val_acc: 0.8359\n",
      "Epoch 286/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3604 - acc: 0.8481 - val_loss: 0.3713 - val_acc: 0.8373\n",
      "Epoch 287/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3598 - acc: 0.8482 - val_loss: 0.3706 - val_acc: 0.8410\n",
      "Epoch 288/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3622 - acc: 0.8497 - val_loss: 0.3718 - val_acc: 0.8418\n",
      "Epoch 289/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3619 - acc: 0.8496 - val_loss: 0.3719 - val_acc: 0.8467\n",
      "Epoch 290/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3611 - acc: 0.8495 - val_loss: 0.3845 - val_acc: 0.8395\n",
      "Epoch 291/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3593 - acc: 0.8470 - val_loss: 0.3671 - val_acc: 0.8382\n",
      "Epoch 292/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3609 - acc: 0.8494 - val_loss: 0.3737 - val_acc: 0.8350\n",
      "Epoch 293/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3614 - acc: 0.8485 - val_loss: 0.3748 - val_acc: 0.8408\n",
      "Epoch 294/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3617 - acc: 0.8460 - val_loss: 0.3950 - val_acc: 0.8455\n",
      "Epoch 295/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3612 - acc: 0.8483 - val_loss: 0.3824 - val_acc: 0.8446\n",
      "Epoch 296/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3648 - acc: 0.8497 - val_loss: 0.3746 - val_acc: 0.8361\n",
      "Epoch 297/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3607 - acc: 0.8485 - val_loss: 0.3712 - val_acc: 0.8423\n",
      "Epoch 298/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3602 - acc: 0.8487 - val_loss: 0.3743 - val_acc: 0.8378\n",
      "Epoch 299/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3586 - acc: 0.8491 - val_loss: 0.3665 - val_acc: 0.8425\n",
      "Epoch 300/300\n",
      "12400/12400 [==============================] - 1s - loss: 0.3596 - acc: 0.8506 - val_loss: 0.3827 - val_acc: 0.8386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11bdc5c88>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\n",
    "batch_size = 10\n",
    "epochs = 300\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[history_cb],verbose=1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 0.8385700846660396\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 0.0032  0.0041  0.0013  0.0374]\n",
      " [ 0.0013  0.369   0.0036  0.0241]\n",
      " [ 0.0002  0.0137  0.029   0.032 ]\n",
      " [ 0.      0.0371  0.0066  0.4374]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "Cm = confusion_matrix(y_eval,pred)\n",
    "C = np.sum(Cm)\n",
    "Cm = Cm/C\n",
    "print('Confusion Matrix:')\n",
    "print(np.array_str(Cm, precision=4, suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/zCBKfLJppJWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b519240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valAcc = history_cb.val_acc\n",
    "valLoss = history_cb.val_loss\n",
    "epoch_it = np.arange(1,301)\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch_it, valAcc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.tight_layout()\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch_it, valLoss)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ffe0668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ls = history_cb.losses\n",
    "Acc = history_cb.accs\n",
    "ntr = np.shape(x_train)[0]\n",
    "epochNum = []\n",
    "for i in range(116400):\n",
    "    epochNum.append(i*10/ntr)\n",
    "\n",
    "plt.subplot(121)\n",
    "matplotlib.pyplot.semilogy(epochNum, Acc)\n",
    "# plt.plot(epochNum, Acc)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(122)\n",
    "matplotlib.pyplot.semilogy(epochNum, Ls)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# plt.axis([0.3,3,0.96,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.07\n",
      "0 1 0.09\n",
      "0 2 0.03\n",
      "0 3 0.81\n",
      "---\n",
      "1 0 0.00\n",
      "1 1 0.93\n",
      "1 2 0.01\n",
      "1 3 0.06\n",
      "---\n",
      "2 0 0.00\n",
      "2 1 0.18\n",
      "2 2 0.39\n",
      "2 3 0.43\n",
      "---\n",
      "3 0 0.00\n",
      "3 1 0.08\n",
      "3 2 0.01\n",
      "3 3 0.91\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# print(Cm[0])\n",
    "nd = [None]*4\n",
    "for i,nd in enumerate (Cm):\n",
    "    for j in range (len(nd)):\n",
    "        \n",
    "        print(i,j,\"%0.2f\"%(nd[j]/np.sum(nd)))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
